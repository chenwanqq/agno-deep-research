{
  "task_description": "è¯·ç ”ç©¶muonä¼˜åŒ–å™¨çš„åŸºæœ¬åŸç†",
  "search_history": [
    {
      "query": "muon optimizer basic principles",
      "timestamp": "2025-09-08T01:21:14.192155",
      "results": [
        {
          "title": "Muon Optimizer: Matrix-Aware Learning - Emergent Mind",
          "url": "https://www.emergentmind.com/topics/muon-optimizer",
          "content": "The Muon optimizer is a geometry-aware, matrix-structured optimization algorithm designed to improve the stability, efficiency, and scalability of large-scale machine learningâ€”especially deep neural network training. Unlike traditional first-order optimizers such as Adam, which treat neural network parameters as flattened vectors and maintain per-coordinate adaptivity, Muon directly exploits the matrix structure of weight parameters, uses spectral norm-based update rules, and implements a form [...] 2000 character limit reached\n\n# Muon Optimizer: Matrix-Aware Learning\n\nUpdated 30 June 2025\n\n Muon Optimizer is a matrix-structured, geometry-aware algorithm that enhances deep learning stability and efficiency.\n It employs spectral norm-based update rules and implicit regularization to control the Lipschitz constant and improve generalization.\n Demonstrated in large-scale LLM pretraining and distributed training, it achieves competitive convergence and reduced memory overhead. [...] Muon operates within the Lion-ğ’¦ family of optimizers by selecting the nuclear norm as its convex map, yielding a matrix-structured update. For a parameter matrix XâˆˆRnÃ—m and gradient Gtâ€‹, Muonâ€™s core update (with decoupled weight decay) is: Mt+1â€‹â€‹=Î²2â€‹Mtâ€‹âˆ’(1âˆ’Î²2â€‹)Gtâ€‹ Pt+1â€‹â€‹=Î²1â€‹Mt+1â€‹âˆ’(1âˆ’Î²1â€‹)Gtâ€‹ Xt+1â€‹â€‹=Xtâ€‹+Î·tâ€‹(âˆ‡K(Pt+1â€‹)âˆ’Xt+1â€‹)â€‹",
          "score": 0.80442166,
          "index": 1
        },
        {
          "title": "Kimi.ai on X: \"Why We Chose Muon: Our Chain of Thought\" / X - Twitter",
          "url": "https://x.com/Kimi_Moonshot/status/1897929976948965870",
          "content": "We need to approach the question of what makes a good optimizer from more fundamental principles. Intuitively, an ideal optimizer should possess two characteristics: stability and speed. Specifically, each update step of an ideal optimizer should: 1) minimize disturbance to the model, and 2) maximize contribution to loss reduction. More directly, we don't want to drastically alter the model (stability) while still significantly reducing the loss (speed)â€”the classic \"wanting to have it both [...] âˆ¥W tâˆ¥=âˆ¥W tâˆ’1âˆ’Î· t(O t+Î» W tâˆ’1)âˆ¥=âˆ¥(1âˆ’Î· t Î»)W tâˆ’1âˆ’Î· t Î»(O t/Î»)âˆ¥â‰¤(1âˆ’Î· t Î»)âˆ¥W tâˆ’1âˆ¥+Î· t Î»âˆ¥O t/Î»âˆ¥â‰¤maxâ¡(âˆ¥W tâˆ’1âˆ¥,âˆ¥O t/Î»âˆ¥)\\begin{equation}\\begin{aligned} \\Vert\\boldsymbol{W}_t\\Vert =&\\, \\Vert\\boldsymbol{W}_{t-1} - \\eta_t (\\boldsymbol{O}_t + \\lambda \\boldsymbol{W}_{t-1})\\Vert \\\\[5pt] =&\\, \\Vert(1 - \\eta_t \\lambda)\\boldsymbol{W}_{t-1} - \\eta_t \\lambda (\\boldsymbol{O}_t/\\lambda)\\Vert \\\\[5pt] \\leq &\\,(1 - \\eta_t \\lambda)\\Vert\\boldsymbol{W}_{t-1}\\Vert + \\eta_t \\lambda \\Vert\\boldsymbol{O}_t/\\lambda\\Vert [...] is some measure of stability, with smaller values indicating greater stability, and Î· is a constant less than 1, representing our stability requirement. As we'll see later, this effectively becomes the learning rate of the optimizer. If readers don't mind, we might borrow a concept from theoretical physics and call this the \"Least Action Principle\" for optimizers.\n\nMatrix Norms\n\nImage 4: Image",
          "score": 0.7892503,
          "index": 2
        },
        {
          "title": "Building the Muon Optimizer in PyTorch: A Geometric Approach to ...",
          "url": "https://medium.com/@kyeg/building-the-muon-optimizer-in-pytorch-a-geometric-approach-to-neural-network-optimization-17f4601be548",
          "content": "Letâ€™s start by understanding the core mathematical principles behind Muon, and then weâ€™ll dive into building a robust implementation that you can use in your everyday deep learning workflows.\n\n# The Theory Behind Muon Optimizer\n\nLetâ€™s break down the key theoretical components that make Muon so effective:\n\n## 1. Metrizing the Linear Layer\n\nLinear layers form the backbone of most neural networks, operating as a simple matrix multiplication:\n\n```\ny = Wx y\n```\n\nWhere: [...] 2. Faster Convergence: Muon consistently demonstrates faster convergence than traditional optimizers like Adam. This acceleration reflects a more efficient traversal of the loss landscape, guided by the geometric principles encoded in the optimizer. [...] for linear layers.        Muon uses a geometric approach to optimization, specifically addressing    how changes in weight matrices affect neural network behavior.        Args:        params (iterable): iterable of parameters to optimize or dicts defining parameter groups        lr (float, optional): learning rate (default: 1e-3)        ns_iters (int, optional): number of Newton-Schulz iterations (default: 5)        momentum (float, optional): momentum factor (default: 0.9)        weight_decay",
          "score": 0.7803451,
          "index": 3
        }
      ],
      "status": "success"
    },
    {
      "query": "Muon optimizer matrix structure optimization",
      "timestamp": "2025-09-08T01:21:44.010610",
      "results": [
        {
          "title": "Muon Optimizer: Matrix-Aware Learning - Emergent Mind",
          "url": "https://www.emergentmind.com/topics/muon-optimizer",
          "content": "The Muon optimizer is a geometry-aware, matrix-structured optimization algorithm designed to improve the stability, efficiency, and scalability of large-scale machine learningâ€”especially deep neural network training. Unlike traditional first-order optimizers such as Adam, which treat neural network parameters as flattened vectors and maintain per-coordinate adaptivity, Muon directly exploits the matrix structure of weight parameters, uses spectral norm-based update rules, and implements a form [...] 2000 character limit reached\n\n# Muon Optimizer: Matrix-Aware Learning\n\nUpdated 30 June 2025\n\n Muon Optimizer is a matrix-structured, geometry-aware algorithm that enhances deep learning stability and efficiency.\n It employs spectral norm-based update rules and implicit regularization to control the Lipschitz constant and improve generalization.\n Demonstrated in large-scale LLM pretraining and distributed training, it achieves competitive convergence and reduced memory overhead. [...] Muon operates within the Lion-ğ’¦ family of optimizers by selecting the nuclear norm as its convex map, yielding a matrix-structured update. For a parameter matrix XâˆˆRnÃ—m and gradient Gtâ€‹, Muonâ€™s core update (with decoupled weight decay) is: Mt+1â€‹â€‹=Î²2â€‹Mtâ€‹âˆ’(1âˆ’Î²2â€‹)Gtâ€‹ Pt+1â€‹â€‹=Î²1â€‹Mt+1â€‹âˆ’(1âˆ’Î²1â€‹)Gtâ€‹ Xt+1â€‹â€‹=Xtâ€‹+Î·tâ€‹(âˆ‡K(Pt+1â€‹)âˆ’Xt+1â€‹)â€‹",
          "score": 0.95261395,
          "index": 1
        },
        {
          "title": "AdaMuon: Adaptive Muon Optimizer - arXiv",
          "url": "https://arxiv.org/html/2507.11005v1",
          "content": "## 2 Preliminary\n\n### 2.1 Muon Optimizer\n\nWe first introduce the Muon optimizer (Jordan et al., 2024), a recent optimization paradigm tailored for two-dimensional parameter structures.\nGiven a weight matrix with the corresponding gradient at iteration , Muon first computes a standard momentum buffer :\n\n|  |  |  |  |\n ---  --- |\n|  |  |  | (1) | [...] Nowadays, as modern large-scale models continue to grow in size and complexity, their parameter spaces exhibit increasingly rich inter-dependencies and matrix-structured geometries.\nThis has motivated growing interest in optimization algorithms that exploit global structure within the gradients to achieve better conditioning and more efficient training dynamics. [...] Inspired by this, we aim to incorporate similar second-moment cues into Muon to enhance its optimization capability.\nNotably, Muon, through its polar decomposition update, inherently preserves structural information across parameter matrices.\nThanks to the identity\n\n|  |  |  |  |\n ---  --- |\n|  |  |  | (5) |",
          "score": 0.85008353,
          "index": 2
        },
        {
          "title": "Understanding the Muon Optimizer: A Game-Changer for Large ...",
          "url": "https://jehillparikh.medium.com/understanding-the-muon-optimizer-a-game-changer-for-large-language-model-training-15316975f0bc",
          "content": "The Muon optimizer, introduced in the arXiv paper, takes a different approach by leveraging matrix orthogonalization â€” a technique that ensures weight updates maintain certain geometric properties in the loss landscape. Unlike RMSprop, Adam, and AdamW, which focus on element-wise gradient updates, Muon considers the structure of weight matrices, making it particularly suited for large-scale LLM training.\n\n## What Makes Muon Stand Out? [...] The Muon optimizer is a bold step forward in the quest to train ever-larger language models efficiently. By combining matrix orthogonalization with practical tweaks like decoupled weight decay, it offers a scalable, efficient alternative to AdamW that requires less hyperparameter fuss. The success of the Moonlight model underscores Muonâ€™s potential to redefine how we train LLMs, making it a must-watch for anyone in the AI community. [...] Muon uses matrix orthogonalization to guide parameter updates, ensuring that weight changes align in a way that stabilizes training. This is a big shift from the moment-based, element-wise updates of Adam and AdamW.\n Think of it as optimizing not just the magnitude of updates (like Adam) but also their direction, which can lead to smoother convergence.\n\nScalability for Large Models:",
          "score": 0.82492816,
          "index": 3
        },
        {
          "title": "Muon is a Nuclear Lion King - UT Computer Science",
          "url": "https://www.cs.utexas.edu/~lqiang/lionk/html/intro.html",
          "content": "| We show that the recently proposed Muon optimizer ([JJB+24], [LSY+25]) is an instance of the Lion- optimizer [CLLL23] when equipped with the nuclear norm. Using the theoretical results of Lion-, this immediately establishes that Muon (with weight decay) solves a constrained optimization problem that limits the maximum singular values of the weight matrices. Additionally, it entails a Lyapunov function for the continuous-time dynamics of the Muon optimizer, thereby confirming its asymptotic [...] for which we can take . Here,  for  denotes the Schatten -norm defined as\n\nwhere  is the th singular value of . In particular, when  or , we have\n\nwhere  is known as the nuclear norm, or trace norm, and  is the spectral norm.\n\nThus, the Muon optimizer solves a constrained optimization on an upper bound on the maximum singular values on the weight matrices:\n\nThe Lyapunov function is\n\n|  |  |  |\n --- \n| (unconstrained) | (constraint initially satisfied) | (constraint initially violated) | [...] The Muon optimizer [JJB+24, LSY+25], with Nesterov momentum and weight decay, has a form of\n\nwhere  is the matrix sign function, defined as",
          "score": 0.80014634,
          "index": 4
        }
      ],
      "status": "success"
    }
  ],
  "total_searches": 2,
  "created_at": "2025-09-08T01:23:10.412175"
}