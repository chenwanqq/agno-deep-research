本报告系统梳理了深度学习发展的两大关键阶段及Transformer架构带来的范式革命，揭示了人工智能从理论奠基到产业爆发的完整演进脉络。

第一阶段（1943–1986年）为理论奠基期：McCulloch-Pitts模型与感知机构建了神经网络的基本框架，但受限于单层结构无法处理非线性问题，导致领域陷入低谷。1986年反向传播算法的提出首次实现了多层网络的有效训练，奠定了深度学习的方法论基础，却因算力与数据不足未能广泛应用。

第二阶段（2006–2012年）为复兴期：Hinton等人提出的深度信念网络（DBN）与堆叠自动编码器（SAE），通过“无监督预训练+有监督微调”策略，成功解决深层网络训练中的梯度消失与初始化难题，首次实证“深度”的有效性，重新点燃学术界信心，并直接催化了2012年AlexNet在ImageNet上的突破。此后，VGGNet、GoogLeNet相继以更深架构与智能模块设计，确立“端到端学习”范式，彻底取代传统人工特征工程，推动计算机视觉全面进入深度学习时代，引爆全球AI研究热潮。

2017年，Transformer架构的提出标志着AI领域的又一次颠覆性转折。其核心创新——自注意力机制，使模型能并行捕获序列中任意长距离依赖关系，彻底克服RNN的串行计算瓶颈与梯度衰减问题。凭借完全并行化结构，Transformer极大提升训练效率，为超大规模模型训练奠定基础。随后，BERT与GPT系列基于Transformer实现NLP性能飞跃，开启大语言模型时代；Vision Transformer（ViT）、CLIP、DALL·E等模型进一步将该架构拓展至视觉与多模态领域，统一图像、文本、音频等异构信息的建模方式，成为新一代AI的通用基础架构。

综上，深度学习的发展经历了从理论突破到实践复兴，再到架构革命的三重跃迁。CNN确立了端到端感知能力，而Transformer则引领了从专用模型向通用大模型的范式转型，共同奠定了现代人工智能的技术基石。