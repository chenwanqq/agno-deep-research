本报告系统梳理了神经网络与深度学习发展的两大关键阶段，厘清了其理论奠基与复兴脉络。

第一阶段（1943–1986年）为理论奠基期。McCulloch与Pitts于1943年提出首个神经元形式化模型，构建了神经网络的计算框架；Rosenblatt于1958年发明感知机，首次实现权重自适应学习，开启机器学习先河；但因单层结构无法解决非线性问题，1969年Minsky与Papert的理论限制分析导致领域陷入长期低谷。直至1986年，Rumelhart、Hinton与Williams在《Nature》上系统提出并推广反向传播算法，首次实现多层网络的有效训练，为深度学习奠定核心训练方法，然受限于算力与数据，未能立即广泛应用。

第二阶段（2006–2012年）为深度学习复兴期。Geoffrey Hinton等人提出的深度信念网络（DBN）与堆叠自动编码器（SAE）引入“贪婪逐层无监督预训练+有监督微调”范式，成功突破深层网络训练中的梯度消失与初始化难题。DBN通过受限玻尔兹曼机（RBM）逐层学习数据特征，SAE则利用确定性自编码器结构实现高效特征提取，二者均能利用海量未标注数据预训练，为网络提供高质量初始参数，使训练深层网络成为现实。尽管存在局部最优、计算复杂或重构过拟合等局限，且最终被CNN、Transformer等架构超越，但它们首次证明了“深度”的有效性，重新点燃学术界信心，吸引大量研究投入，直接催化了2012年AlexNet等端到端监督模型的爆发式成功。

综上，从1943年的结构雏形、1986年的训练突破，到2006年的预训练革命，这三项里程碑共同构建了现代深度学习的完整理论基石，推动人工智能从理论构想迈向实践前沿。