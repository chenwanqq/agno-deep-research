本报告系统梳理了1943年至1986年间神经网络领域的三项奠基性工作，厘清了深度学习的理论起源与发展脉络。

首先，McCulloch与Pitts于1943年提出的神经元模型，首次将生物神经元抽象为二进制逻辑单元，建立了神经网络的形式化计算框架，证明了其可实现任意布尔逻辑运算，为后续研究提供了核心概念基础，但该模型缺乏学习能力且过于简化。

其次，Rosenblatt于1958年提出的感知机是首个具备学习能力的神经网络模型，通过权重调整实现线性分类，标志着机器学习的开端。然而，其单层结构仅能处理线性可分问题，1969年Minsky与Papert对XOR等非线性问题的不可解性证明，直接导致神经网络研究陷入长达二十年的低谷。

最后，尽管反向传播算法的思想早有萌芽，但Rumelhart、Hinton与Williams于1986年在《Nature》发表的论文首次系统阐述并推广了该算法在多层神经网络中的应用，实现了对隐藏层的有效训练，突破了感知机的结构限制，成为神经网络复兴的关键转折点。然而，受限于当时计算资源匮乏、正则化手段缺失及优化困难，该算法并未立即广泛应用，其真正潜力直至数十年后算力与数据条件成熟才得以释放。

综上，这三项工作分别奠定了神经网络的结构基础、学习机制与训练方法，共同构建了现代深度学习的理论基石。