本报告系统梳理了深度学习与大型语言模型（LLM）的发展脉络，揭示了人工智能从理论奠基到范式革命的完整演进路径。

深度学习历经三重跃迁：1943–1986年为理论奠基期，反向传播算法奠定多层网络训练基础；2006–2012年为复兴期，深度信念网络与AlexNet等突破证实“深度”有效性，推动计算机视觉全面进入端到端学习时代；2017年Transformer架构的提出则引发根本性范式变革——自注意力机制实现并行化长距离依赖建模，彻底超越RNN局限，催生BERT、GPT、ViT、CLIP等统一多模态建模的基础模型，确立其作为新一代AI通用架构的核心地位。

2018–2025年间，LLM发展呈现“规模驱动、范式转型、开源赋能”三大特征。GPT系列从参数量级跃迁（GPT-3达1750亿）开启上下文学习能力，后期转向架构优化与效率提升；Google PaLM与PaLM 2依托Pathways系统与高质量数据，在相对更小规模下实现卓越性能，凸显系统设计与数据质量的关键作用；Meta LLaMA系列通过开源模型与指令微调（Instruction Tuning）范式，结合千万级人类标注数据，显著提升对齐能力，带动全球研究生态繁荣。指令微调取代传统任务微调，使模型具备零样本泛化与指令遵循能力，成为现代LLM的核心训练范式。

最关键发现是“涌现能力”——当模型规模跨越临界阈值时，复杂推理、代码生成、心智理论等在小模型中不存在的能力突然显现，证明规模增长带来质变，颠覆了传统线性提升认知。综上，CNN奠定了感知能力，Transformer构建了通用架构，而规模驱动下的指令微调与涌现现象，则共同塑造了当前大模型时代的智能形态与技术基石。