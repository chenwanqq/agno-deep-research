本报告系统梳理了深度学习与大型语言模型（LLM）的发展脉络，揭示了人工智能从理论奠基到范式革命的完整演进路径。深度学习历经三重跃迁：1943–1986年反向传播算法奠定理论基础；2006–2012年深度信念网络与AlexNet推动“深度”有效性验证，开启端到端学习时代；2017年Transformer架构的提出则引发根本性变革，其自注意力机制突破RNN局限，催生BERT、GPT、ViT、CLIP等统一多模态基础模型，确立为新一代AI通用架构。

2018–2025年间，LLM发展呈现“规模驱动、范式转型、开源赋能”三大特征。GPT系列通过参数量级跃升（如GPT-3达1750亿）激活上下文学习能力；Google PaLM 2则证明高质量数据与系统设计（如Pathways）可在更小规模下实现卓越性能；Meta LLaMA系列通过开源模型与指令微调（Instruction Tuning）范式，结合千万级人类标注数据，显著提升对齐能力，带动全球研究生态繁荣。指令微调取代传统任务微调，使模型具备零样本泛化与指令遵循能力，成为现代LLM的核心训练范式。

最关键发现是“涌现能力”——当模型规模跨越临界阈值时，复杂推理、代码生成、心智理论等在小模型中不存在的能力突然显现，证明规模增长带来质变，颠覆线性提升认知。CNN奠定了感知能力，Transformer构建了通用架构，而规模驱动下的指令微调与涌现现象，则共同塑造了当前大模型时代的智能形态。

报告进一步指出，深度学习的崛起是硬件、数据与开源框架协同演进的系统性成果：GPU与TPU等专用硬件提供算力支撑，ImageNet、Common Crawl等大规模数据集成为训练燃料，TensorFlow与PyTorch等开源框架降低门槛并构建生态，三者形成自我强化的“飞轮效应”，共同推动模型规模扩张与能力跃迁，最终催生出具有涌现智能的大模型时代。