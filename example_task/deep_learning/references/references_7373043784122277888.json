[
  {
    "id": 1,
    "gmt_created": "2025-09-15T01:23:48.490195",
    "gmt_modified": "2025-09-15T01:23:48.490195",
    "task_id": "7373043784122277888",
    "task_no": 1,
    "title": "A Logical Calculus of the Ideas Immanent in Nervous Activity",
    "type": "search_result",
    "content": "\"A Logical Calculus of the Ideas Immanent in Nervous Activity\" is a 1943 article written by Warren McCulloch and Walter Pitts.( The paper, published in the journal _The Bulletin of Mathematical Biophysics,_ proposed a mathematical model of the nervous system as a network of simple logical elements, later known as artificial neurons, or McCulloch-Pitts neurons. These neurons receive inputs, perform a weighted sum, and fire an output signal based on a threshold function. By connecting these units [...] 1.   ^McCulloch, Warren S.; Pitts, Walter (December 1943). \"A logical calculus of the ideas immanent in nervous activity\". _The Bulletin of Mathematical Biophysics_. 5 (4): 115–133. doi \"Doi (identifier)\"):10.1007/BF02478259. ISSN \"ISSN (identifier)\")0007-4985.\n2.   ^ _a__b_von Neumann, J. (1951). _The general and logical theory of automata_. In L. A. Jeffress (Ed.), _Cerebral mechanisms in behavior; the Hixon Symposium_ (pp. 1–41). Wiley. [...] In 1938, at age 15, Pitts ran away from home in Detroit and arrived in the University of Chicago. Later, he walked into Rudolf Carnap's office with Carnap's book filled with corrections and suggested improvements. He started studying under Carnap and attending classes during 1938--1943. He wrote several early papers on neuronal network modelling and regularly attended Rashevsky's seminars in theoretical biology. The seminar attendants included Gerhard von Bonin and Householder. In 1940, von",
    "url": "https://en.wikipedia.org/wiki/A_Logical_Calculus_of_the_Ideas_Immanent_in_Nervous_Activity",
    "ext_info": null
  },
  {
    "id": 2,
    "gmt_created": "2025-09-15T01:23:48.499062",
    "gmt_modified": "2025-09-15T01:23:48.499062",
    "task_id": "7373043784122277888",
    "task_no": 2,
    "title": "The First Neuron M-P Model (1943) - LinkedIn",
    "type": "search_result",
    "content": "\"A Logical Calculus of Ideas Immanent in Nervous Activity\" is a landmark paper co-authored by Walter Pitts and Warren McCulloch, published in 1943. This paper is a foundational work in the field of computational neuroscience and artificial intelligence. Here's a description of its key aspects: [...] 1. Theoretical Framework: The paper presents a theoretical framework for understanding how neurons in the brain might process information and make decisions. It proposes a mathematical model of a simplified artificial neuron, now known as the McCulloch-Pitts neuron model. [...] Walter Harry Pitts was an American logician renowned for his pioneering work in computational neuroscience. His groundbreaking theoretical contributions to neural activity and generative processes had a profound impact on diverse fields, including cognitive sciences, psychology, philosophy, neurosciences, computer science, artificial neural networks, cybernetics, and artificial intelligence. He is most celebrated for co-authoring a seminal paper with Warren McCulloch titled \"A Logical Calculus",
    "url": "https://www.linkedin.com/pulse/first-neuron-m-p-model-1943-sayed-qasim",
    "ext_info": null
  },
  {
    "id": 3,
    "gmt_created": "2025-09-15T01:23:48.505180",
    "gmt_modified": "2025-09-15T01:23:48.505180",
    "task_id": "7373043784122277888",
    "task_no": 3,
    "title": "McCulloch-Pitts Neurons - The Mind Project",
    "type": "search_result",
    "content": "|  | MODULE DESCRIPTION: In 1943 Warren S. McCulloch, a neuroscientist, and Walter Pitts, a logician, published \"A logical calculus of the ideas immanent in nervous activity\" in the Bulletin of Mathematical Biophysics 5:115-133. In this paper McCulloch and Pitts tried to understand how the brain could produce highly complex patterns by using many basic cells that are connected together. These basic brain cells are called neurons, and McCulloch and Pitts gave a highly simplified model of a [...] neuron in their paper. The McCulloch and Pitts model of a neuron, which we will call an MCP neuron for short, has made an important contribution to the development of artificial neural networks -- which model key features of biological neurons.  The original MCP Neurons had limitations. Additional features were added which allowed them to \"learn.\" The next major development in neural networks was the concept of a perceptron which was introduced by Frank Rosenblatt in 1958. Essentially the",
    "url": "https://mind.ilstu.edu/curriculum/mcp_neurons/index.html",
    "ext_info": null
  },
  {
    "id": 4,
    "gmt_created": "2025-09-15T01:23:48.510879",
    "gmt_modified": "2025-09-15T01:23:48.510879",
    "task_id": "7373043784122277888",
    "task_no": 4,
    "title": "the intellectual origins of the McCulloch-Pitts neural networks",
    "type": "search_result",
    "content": "This article examines the intellectual and institutional factors that contributed to the collaboration of neuropsychiatrist Warren McCulloch and mathematician Walter Pitts on the logic of neural networks, which culminated in their 1943 publication, \"A Logical Calculus of the Ideas Immanent in Nervous Activity.\" Historians and scientists alike often refer to the McCulloch-Pitts paper as a landmark event in the history of cybernetics, and fundamental to the development of cognitive science and [...] artificial intelligence. This article seeks to bring some historical context to the McCulloch-Pitts collaboration itself, namely, their intellectual and scientific orientations and backgrounds, the key concepts that contributed to their paper, and the institutional context in which their collaboration was made. Although they were almost a generation apart and had dissimilar scientific backgrounds, McCulloch and Pitts had similar intellectual concerns, simultaneously motivated by issues in [...] philosophy, neurology, and mathematics. This article demonstrates how these issues converged and found resonance in their model of neural networks. By examining the intellectual backgrounds of McCulloch and Pitts as individuals, it will be shown that besides being an important event in the history of cybernetics proper, the McCulloch-Pitts collaboration was an important result of early twentieth-century efforts to apply mathematics to neurological phenomena.",
    "url": "https://pubmed.ncbi.nlm.nih.gov/11835218/",
    "ext_info": null
  },
  {
    "id": 5,
    "gmt_created": "2025-09-15T01:23:53.547807",
    "gmt_modified": "2025-09-15T01:23:53.547807",
    "task_id": "7373043784122277888",
    "task_no": 5,
    "title": "Frank Rosenblatt's Perceptron, Birth of The Neural Network - Medium",
    "type": "search_result",
    "content": "Frank Rosenblatt’s perceptron was a pioneering achievement in the history of artificial intelligence. While it had limitations, it introduced the concept of learning to machines and laid the groundwork for subsequent developments in neural networks and machine learning. The perceptron serves as a testament to the importance of simple, foundational ideas in driving progress in AI. Today, neural networks, including deep learning models, owe their existence to the humble beginnings of the [...] applications. While the perceptron showed promise, it was limited in its capabilities, particularly in handling non-linear problems. This limitation led to skepticism and decreased interest in neural networks, known as the “AI winter.” [...] Now, we need to create an instance of the perceptron class. Set the epochs to 10 and the learning rate to 0.1. Once the instance is created, we fit the model using the training partitions.",
    "url": "https://medium.com/@robdelacruz/frank-rosenblatts-perceptron-19fcce9d627f",
    "ext_info": null
  },
  {
    "id": 6,
    "gmt_created": "2025-09-15T01:23:53.554542",
    "gmt_modified": "2025-09-15T01:23:53.554542",
    "task_id": "7373043784122277888",
    "task_no": 6,
    "title": "Perceptron - Wikipedia",
    "type": "search_result",
    "content": "Rosenblatt described the details of the perceptron in a 1958 paper. His organization of a perceptron is constructed of three kinds of cells (\"units\"): AI, AII, R, which stand for \"projection\", \"association\" and \"response\". He presented at the first international symposium on AI, Mechanisation of Thought Processes, which took place in 1958 November. [...] 10. ^ Rosenblatt, F. (1958). \"The perceptron: A probabilistic model for information storage and organization in the brain\". Psychological Review. 65 (6): 386–408. doi \"Doi (identifier)\"):10.1037/h0042519. ISSN \"ISSN (identifier)\") 1939-1471. PMID \"PMID (identifier)\") 13602029. [...] Aizerman, M. A. and Braverman, E. M. and Lev I. Rozonoer. Theoretical foundations of the potential function method in pattern recognition learning. Automation and Remote Control, 25:821–837, 1964.\n Rosenblatt, Frank (1958), The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain, Cornell Aeronautical Laboratory, Psychological Review, v65, No. 6, pp. 386–408. doi \"Doi (identifier)\"):10.1037/h0042519.",
    "url": "https://en.wikipedia.org/wiki/Perceptron",
    "ext_info": null
  },
  {
    "id": 7,
    "gmt_created": "2025-09-15T01:23:53.561338",
    "gmt_modified": "2025-09-15T01:23:53.561338",
    "task_id": "7373043784122277888",
    "task_no": 7,
    "title": "The Limitations of Perceptron: Why it Struggles with XOR - Medium",
    "type": "search_result",
    "content": "Perceptrons, introduced by Frank Rosenblatt in the late 1950s, represent one of the simplest forms of artificial neural networks. They are binary classifiers that use a linear decision boundary to classify input data into one of two categories. Despite their historical importance as the building blocks of neural networks, perceptrons have several significant limitations that have led to the development of more advanced neural network models. [...] One of the most fundamental limitations of perceptrons is their inability to solve problems that are not linearly separable. A perceptron can only draw a straight line (or hyperplane in higher dimensions) to separate two classes. If the data points from different classes cannot be separated by a straight line, the perceptron fails to classify them correctly. [...] Perceptrons were important in the early days of neural networks, but they have clear limitations. They can’t solve complex problems, are sensitive to input data, and can only handle simple, linear tasks. Their single-layer structure and lack of memory make them unsuitable for more advanced tasks, and they struggle with multi-class problems and large datasets. Although newer models have taken their place, understanding perceptrons helps us appreciate the progress in AI and the need for more",
    "url": "https://medium.com/@aryanrusia8/the-limitations-of-perceptron-why-it-struggles-with-xor-21905d31f924",
    "ext_info": null
  },
  {
    "id": 8,
    "gmt_created": "2025-09-15T01:23:53.567725",
    "gmt_modified": "2025-09-15T01:23:53.567725",
    "task_id": "7373043784122277888",
    "task_no": 8,
    "title": "The Rosenblatt's Perceptron - - Maël Fabien",
    "type": "search_result",
    "content": "The inputs can be seen as neurons and will be called the input layer. Altogether, these neurons and the function (which we’ll cover in a minute) form a perceptron.\n\nHow do we make classification using a perceptron then?\n\ny=1 if ∑iwixi≥0, else y=0\n\nOne limitation remains: the inputs need to be linearly separable since we split the input space into two halves.\n\n## Minsky and Papert (1969) [...] OR: the `f` function checks if the sum `g` is equal to 1\n AND: the `f` function checks if the sum `g` is equal to the number of inputs\n GREATER THAN: the `f` function checks if the sum `g` is equal to a threshold θ\n\nThe simplest binary classification can be achieved the following way :\n\ny=1 if ∑ixi≥0, else y=0\n\nThere are however several limitations to McCulloch-Pitts Neurons : [...] The perceptron was first introduced in 1957 by Franck Rosenblatt. Since then, it has been the core of Deep Learning. We can represent schematically a perceptron as :\n\nWe attach to each input a weight ( wi) and notice how we add an input of value 1 with a weight of −θ. This is called bias. What we are doing is instead of having only the inputs and the weight and compare them to a threshold, we also learn the threshold as a weight for a standard input of value 1.",
    "url": "https://maelfabien.github.io/deeplearning/Perceptron/",
    "ext_info": null
  },
  {
    "id": 9,
    "gmt_created": "2025-09-15T01:23:58.462935",
    "gmt_modified": "2025-09-15T01:23:58.462935",
    "task_id": "7373043784122277888",
    "task_no": 9,
    "title": "Backpropagation - Algorithm Hall of Fame",
    "type": "search_result",
    "content": "Backpropagation – Algorithm Hall of Fame\n\nThe backpropagation algorithm was originally introduced in the 1970s, but its importance wasn’t fully appreciated until a famous 1986 paper by David Rumelhart, Geoffrey Hinton, and Ronald Williams. That paper describes several neural networks where backpropagation works far faster than earlier approaches to learning, making it possible to use neural nets to solve problems which had previously been insoluble. [...] The firefly algorithm is a metaheuristic proposed by Xin-She Yang and inspired by the flashing behaviour of fireflies. The primary…\n\n### Latest nominations",
    "url": "https://www.algorithmhalloffame.org/algorithms/neural-networks/backpropagation/",
    "ext_info": null
  },
  {
    "id": 10,
    "gmt_created": "2025-09-15T01:23:58.472951",
    "gmt_modified": "2025-09-15T01:23:58.472951",
    "task_id": "7373043784122277888",
    "task_no": 10,
    "title": "A short history of the algorithm that changed science - LinkedIn",
    "type": "search_result",
    "content": "Explicit, efficient error backpropagation in arbitrary neural networks-like networks was first described in a 1970 master's thesis (Linnainmaa, 1970, 1976), albeit without explicit reference to NNs, when Linnainmaa introduced the reverse mode of automatic differentiation to efficiently compute the derivative of a differentiable composite function that can be represented as a graph, by recursively applying the chain rule to the function’s building blocks. [...] It was Paul John Werbos, an American social scientist and machine learning pioneer, who first described the process of training artificial neural networks through backpropagation of errors in his 1974 PhD dissertation. In order to implement dynamic programming in a local biologically plausible manner, he translated Freud’s theory of \"psychic energy\" into an algorithm and a rigorous general theorem, the chain rule for ordered derivatives, which later became known as the method for automatic [...] Backpropagation was proposed in its current form in 1986 by Rumelhart, Hinton and Williams in their famous Nature paper \"Learning Representations by Back-propagating Errors\". Of the three authors, Geoffrey Hinton was the one who carried the flag of machine learning during the many \"AI winters\", until the eternal spring finally broke out in 2012, thanks to another paper he co-authored. For these reasons, one may tend to assume that he was the only mind behind backpropagation. Far from wanting to",
    "url": "https://www.linkedin.com/pulse/short-history-algorithm-changed-science-dr-alessandro-fontana",
    "ext_info": null
  },
  {
    "id": 11,
    "gmt_created": "2025-09-15T01:23:58.480322",
    "gmt_modified": "2025-09-15T01:23:58.480322",
    "task_id": "7373043784122277888",
    "task_no": 11,
    "title": "The Development of Backpropagation - Birow",
    "type": "search_result",
    "content": "Although the theoretical foundations of backpropagation were laid in the 1960s and 1970s, its first truly impactful practical application emerged in the late 1980s. Yann LeCun, now recognized as one of the pioneers of AI, demonstrated the power of combining backpropagation with \\\\Convolutional Neural Networks (CNNs)\\\\ while working at Bell Labs in 1989. LeCun and his colleagues applied CNNs trained with backpropagation to the task of \\\\handwritten digit recognition\\\\. [...] The modern, efficient form of the algorithm, known as \\\\reverse-mode automatic differentiation\\\\, was rigorously described by Finnish researcher Seppo Linnainmaa in his 1970 master's thesis (published 1976). Linnainmaa developed this general method for efficiently calculating derivatives of complex, nested functions, which forms the mathematical core of backpropagation today. However, Linnainmaa's work did not focus on neural networks. [...] Although an indispensable tool in modern artificial intelligence and machine learning, the history of backpropagation spans several decades and involves numerous scientific breakthroughs. Its development began conceptually in the 1960s, but it only gained widespread prominence in the mid-1980s after foundational contributions and popularization by researchers such as Frank Rosenblatt, Seppo Linnainmaa, and Paul Werbos.\n\n### Early Theoretical Foundations",
    "url": "https://www.birow.com/backpropagation",
    "ext_info": null
  },
  {
    "id": 12,
    "gmt_created": "2025-09-15T01:23:58.487999",
    "gmt_modified": "2025-09-15T01:23:58.487999",
    "task_id": "7373043784122277888",
    "task_no": 12,
    "title": "Why did it take so long to invent the backpropagation algorithm? Isn't ...",
    "type": "search_result",
    "content": "The backpropagation algorithm was first formulated in 1974, in the PhD thesis of Paul Werbos (supposedly, I haven’t actually found a copy of the thesis yet, please comment if you know where to find it).\n\nSo, is five years “so long”? [...] > The first major extension of the feedforward neural network beyond Madaline I took place in 1971 when Werbos developed a backpropagation training algorithm which, in 1974, he first published in his doctoral dissertation [371.’ Unfortunately, Werbos’s work remained almost unknown in the scientific community. In 1982, Parker rediscovered the technique  and in 1985, published a report on it at M.I.T. . Not long after Parker published his findings, Rumelhart, Hinton, and Williams ,  also [...] 2) The reverse mode of auto-differentiation or automatic differentiation for gradient computation and backpropagation (1). The mechanism seems to be first introduced by the Finnish Seppo Linnainmaa in 1974 and rediscovered by Parker in 1982, LeCun in a French unpublished paper around the same date, Hinton and Rumelhart around 1985. This is the usual definition of backpropagation for neural networks but can be used anywhere we need to compute gradient in a computational graph (including in other",
    "url": "https://www.quora.com/Why-did-it-take-so-long-to-invent-the-backpropagation-algorithm-Isnt-it-just-a-straightforward-albeit-cumbersome-application-of-the-chain-rule",
    "ext_info": null
  },
  {
    "id": 13,
    "gmt_created": "2025-09-15T01:24:03.363155",
    "gmt_modified": "2025-09-15T01:24:03.363155",
    "task_id": "7373043784122277888",
    "task_no": 13,
    "title": "Rumelhart et al. (1986) - Backpropagation - Chat Overview",
    "type": "search_result",
    "content": "## Chat Overview\n\n# Rumelhart et al. (1986) - Backpropagation\n\nThe 1986 revival of backpropagation by Rumelhart, Hinton, and Williams enabled deep learning by allowing neural networks to learn internal representations through gradient-based optimization. [...] The 1986 paper by David Rumelhart, Geoffrey Hinton, and Ronald Williams marked a turning point in the history of neural networks. Their work reintroduced and popularized the backpropagation algorithm, which made it possible to train multi-layer neural networks - overcoming the severe limitations of the earlier Perceptronmodel.\n\nBy enabling internal layers to learn distributed representations, backpropagation laid the mathematical foundation for modern deep learning.\n\n## Historical Context [...] Rumelhart et al. built on earlier ideas from control theory and automatic differentiation to formalize an efficient way to compute gradients through multiple layers of weights. Their 1986 paper, Learning representations by back-propagating errors, demonstrated empirically that neural networks could be trained to perform nontrivial tasks by adjusting weights through error propagation.\n\n## Technical Summary",
    "url": "https://chatoverview.com/foundations/papers-architectures/backpropagation/",
    "ext_info": null
  },
  {
    "id": 14,
    "gmt_created": "2025-09-15T01:24:03.372719",
    "gmt_modified": "2025-09-15T01:24:03.372719",
    "task_id": "7373043784122277888",
    "task_no": 14,
    "title": "What is Backpropagation? | IBM",
    "type": "search_result",
    "content": "Though equivalents and predecessors to backpropagation were independently proposed in varying contexts dating back to the 1960s, David E. Rumelhart, Geoffrey Hinton and Ronald J. Williams first published the formal learning algorithm. Their 1986 paper, “Learning representations by back-propagating errors,” provided the derivation of the backpropagation algorithm as used and understood in a modern machine learning context.",
    "url": "https://www.ibm.com/think/topics/backpropagation",
    "ext_info": null
  },
  {
    "id": 15,
    "gmt_created": "2025-09-15T01:24:03.381192",
    "gmt_modified": "2025-09-15T01:24:03.381192",
    "task_id": "7373043784122277888",
    "task_no": 15,
    "title": "From Brain to Machine: The Unexpected Journey of Neural Networks",
    "type": "search_result",
    "content": "Next came the coup de gras: The backpropagation algorithm that Rumelhart, Hinton and Williams presented in _Nature,_ also in 1986. [...] Despite the elegance of the backpropagation algorithm, neural network models didn’t immediately take off. Indeed, it wasn’t until 25 years later that Hinton and his students leveraged Fei-Fei Li’s ImageNet dataset — using computers that were many orders of magnitude more powerful than the computers Rumelhart had at his disposal — to demonstrate convolutional neural networks’ impressive ability to classify images. “Before then, it was very hard to train networks that were deep enough or had [...] At the time, Rumelhart was using backpropagation with networks that had a very small number of input units and one layer of units in between the inputs and the output, McClelland says. By contrast, today’s models may have thousands of intermediate layers of neurons that are learning the same way.",
    "url": "https://hai.stanford.edu/news/brain-machine-unexpected-journey-neural-networks",
    "ext_info": null
  },
  {
    "id": 16,
    "gmt_created": "2025-09-15T01:24:03.390157",
    "gmt_modified": "2025-09-15T01:24:03.390157",
    "task_id": "7373043784122277888",
    "task_no": 16,
    "title": "Backpropagation: The Core of Neural Networks and Geoffrey ...",
    "type": "search_result",
    "content": "David E. Rumelhart and his team through their important 1986 paper, “Learning Representations by Back-Propagating Errors”. This work showed how to use backpropagation to train multi-layer perceptrons (MLPs) and helped bring back interest in neural networks.\n\nThe algorithm itself was first introduced by Seppo Linnainmaa in 1970 as the reverse mode of automatic differentiation. However, it was the 1986 paper that really made it a big part of machine learning research.\n\nReferences: [...] The idea of backpropagation was discovered by several researchers in the 1970s. One of the first people to talk about it was Paul Werbos in his 1974 PhD thesis. Around the same time, other researchers like Seppo Linnainmaa and David Rumelhart, along with Geoffrey Hinton and Ronald Williams, also worked on backpropagation in neural networks. [...] Hinton’s contributions to the field of ANNs were instrumental in making backpropagation accessible and practical for a wide range of applications. His efforts and thought leadership played a pivotal role in mainstreaming the use of ANNs across industries.",
    "url": "https://medium.com/@don-lim/backpropagation-the-core-of-neural-networks-and-geoffrey-hintons-contribution-6e3caddbc8cb",
    "ext_info": null
  },
  {
    "id": 17,
    "gmt_created": "2025-09-15T01:24:03.398085",
    "gmt_modified": "2025-09-15T01:24:03.398085",
    "task_id": "7373043784122277888",
    "task_no": 17,
    "title": "Geoffrey Hinton, The God Father Of Deep Learning And Neural ...",
    "type": "search_result",
    "content": "Hinton’s research in the 1980s, in collaboration with David Rumelhart and Ronald Williams, led to the development of a fast, practical method for implementing backpropagation in neural networks (Rumelhart et al., 1986). [...] In the 1980s, G Hinton, along with David Rumelhart and Ronald J. Williams, developed a simplified model of the brain called a neural network. They introduced the backpropagation algorithm, a method for training neural networks that has since become a standard in machine learning. This work was published in the seminal paper “Learning representations by back-propagating errors” (Rumelhart et al., 1986). [...] ## The Birth of Backpropagation: Hinton’s Pioneering Work\n\nBackpropagation, a method used in artificial intelligence (AI) and machine learning, was pioneered by Geoffrey Hinton in the 1980s. This algorithm, fundamental to neural network training, is based on the mathematical concept of gradient descent. It allows the adjustment of weights in a neural network by propagating the error backward from the output layer to the input layer, hence the term ‘backpropagation’ (Rumelhart et al., 1986).",
    "url": "https://quantumzeitgeist.com/geoffrey-hinton/",
    "ext_info": null
  },
  {
    "id": 18,
    "gmt_created": "2025-09-15T01:24:44.385989",
    "gmt_modified": "2025-09-15T01:24:44.385989",
    "task_id": "7373043784122277888",
    "task_no": 18,
    "title": "[PDF] Why Does Unsupervised Pre-training Help Deep Learning?",
    "type": "search_result",
    "content": "The breakthrough to effective training strategies for deep architectures came in 2006 with the algorithms for training deep belief networks (DBN) (Hinton et al., 2006) and stacked auto-encoders (Ranzato et al., 2007; Bengio et al., 2007), which are all based on a similar approach: greedy layer-wise unsupervised pre-training followed by supervised ﬁne-tuning. Each layer is pre-trained with an unsupervised learning algorithm, learning a nonlinear transformation of its input (the output of the [...] We shall consider two deep architectures as representatives of two families of models encoun-tered in the deep learning literature.\n5.1.1 DEEP BELIEF NETWORKS The ﬁrst model is the Deep Belief Net (DBN) by Hinton et al. (2006), obtained by training and stacking several layers of Restricted Boltzmann Machines (RBM) in a greedy manner. Once this stack of RBMs is trained, it can be used to initialize a multi-layer neural network for classiﬁcation. [...] With either DBN or SDAE, an output logistic regression layer is added after unsupervised training. This layer uses softmax (multinomial logistic regression) units to estimate P(class|x) = softmaxclass(a), where ai is a linear combination of outputs from the top hidden layer. The whole network is then trained as usual for multi-layer perceptrons, to minimize the output (negative log-likelihood) prediction error.",
    "url": "https://www.jmlr.org/papers/volume11/erhan10a/erhan10a.pdf",
    "ext_info": null
  },
  {
    "id": 19,
    "gmt_created": "2025-09-15T01:24:44.410196",
    "gmt_modified": "2025-09-15T01:24:44.410196",
    "task_id": "7373043784122277888",
    "task_no": 19,
    "title": "Discover the Power of Deep Belief Networks - Viso Suite",
    "type": "search_result",
    "content": "The pre-training phase aims to initialize the DBN so that the trained weights directly represent the input data. We perform this pre-training phase with unsupervised learning and train each RBM module independently as a feature detector. [...] Deep Belief Networks are constructed by stacking multiple Restricted Boltzmann Machines.\n We train each RBM in the stack independently with greedy learning.\n Training DBNs consists of an unsupervised pre-training phase followed by supervised fine-tuning.\n DBNs understand latent data representations and cangenerate new data samples.\n DBNs are also employed in classification, motion capture, speech recognition, etc. [...] A lower energy means a higher probability of association between the units. By minimizing the energy value for the overall network, the RBM constructs a plausible representation of the original data.\n\n## DBN training\n\nThe training of a deep belief network consists of a pre-training phase and then task-specific fine-tuning. The two methodologies are a hybrid of unsupervised and supervised learning approaches.\n\n### Pre-training",
    "url": "https://viso.ai/deep-learning/deep-belief-networks/",
    "ext_info": null
  },
  {
    "id": 20,
    "gmt_created": "2025-09-15T01:24:44.430153",
    "gmt_modified": "2025-09-15T01:24:44.430153",
    "task_id": "7373043784122277888",
    "task_no": 20,
    "title": "What is the difference between a neural network and a deep belief ...",
    "type": "search_result",
    "content": "usεr11852usεr11852\n\n48.4k44 gold badges113113 silver badges175175 bronze badges\n\n2\n\n 1\n\n  A deep BELIEF network usually refers to a deep network with unsupervised pretraining (stacked restricted Boltzmann machines trained with contrastive divergence).\n\n  alfa\n\n  – \n  alfa\n\n  03/06/2013 13:21:34\n\n  Commented\n  Mar 6, 2013 at 13:21\n 5 [...] In 2006 Hinton discovered that much better results could be achieved in deeper architectures when each layer (RBM) is pre-trained with an unsupervised learning algorithm (Contrastive Divergence). Then the Network can be trained in a supervised way using backpropagation in order to \"fine-tune\" the weights.\n\nShare\n\nCC BY-SA 3.0\n\nImprove this answer\n\nFollow this answer to receive notifications\n\nanswered May 23, 2013 at 16:18\n\nDavid BuchacaDavid Buchaca\n\n74666 silver badges55 bronze badges\n\n1 [...] As you have pointed out a deep belief network has undirected connections between some layers. This means that the topology of the DNN and DBN is different by definition.\n\nThe undirected layers in the DBN are called Restricted Boltzmann Machines. This layers can be trained using an unsupervised learning algorithm (Contrastive Divergence) that is very fast (Here's a link! with details).\n\nSome more comments:",
    "url": "https://stats.stackexchange.com/questions/51273/what-is-the-difference-between-a-neural-network-and-a-deep-belief-network",
    "ext_info": null
  },
  {
    "id": 21,
    "gmt_created": "2025-09-15T01:24:51.238590",
    "gmt_modified": "2025-09-15T01:24:51.238590",
    "task_id": "7373043784122277888",
    "task_no": 21,
    "title": "Unsupervised Pre-training of a Deep LSTM-based Stacked ... - Nature",
    "type": "search_result",
    "content": "Recently, Saikia et al. empirically analysed the effect of different unsupervised pre-training approaches in modelling regression problems using four different datasets30, 342–349 (2018).\"). They considered two methods separately, namely, DBN and stacked autoencoder (SA), and compared their performance with a standard ANN algorithm without pre-training. Liu et al. applied Hinton’s model on stacked autoencoders (SAEs) to solve gearbox fault diagnosis37.\"). The proposed method can directly [...] These results clearly show the impact of stacking LSTMs in an autoencoder fashion as a kind of pre-trained DLSTM, and its performance is better than the original DLSTM that depended on randomized initialization to the units of the LSTM. In other words, the unsupervised pre-training approach enhances the DLSTM’s performance better than the supervised approach of the DLSTM5.\"). The reason is because UTS problems usually include one variable or one feature, where there is no correlation with other [...] a pre-trained LSTM-based stacked autoencoder (LSTM-SAE) approach in an unsupervised learning fashion to replace the random weight initialization strategy adopted in deep LSTM recurrent networks. For evaluation purposes, two different case studies that include real-world datasets are investigated, where the performance of the proposed approach compares favourably with the deep LSTM approach. In addition, the proposed approach outperforms several reference models investigating the same case",
    "url": "https://www.nature.com/articles/s41598-019-55320-6",
    "ext_info": null
  },
  {
    "id": 22,
    "gmt_created": "2025-09-15T01:24:51.248115",
    "gmt_modified": "2025-09-15T01:24:51.248115",
    "task_id": "7373043784122277888",
    "task_no": 22,
    "title": "[PDF] Stacked Similarity-Aware Autoencoders - IJCAI",
    "type": "search_result",
    "content": "More recently, autoencoders have played a key role in the deep neural network based approaches [Hinton and Salakhut-dinov, 2006] where autoencoders are stacked and trained bot-tom up in unsupervised fashion, followed by a supervised learning phase to train the top layer and ﬁne-tune the entire architecture. Unfortunately, if the encoder and decoder are allowed too much capacity, the autoencoder can learn to per-form the copying task without extracting useful feature rep-resentations [Goodfellow [...] input. Among massive unsupervised learning models, the au-toencoder could be stacked to build a deep structure easily and then utilized to generate useful features naturally [Hin-ton and Salakhutdinov, 2006]. Not surprisingly, autoencoder based models have been shown to achieve state-of-the-art per-formance in a number of challenging problems ranging from computer vision [Wang et al., 2016] and audio processing [Pl-chot et al., 2016] to natural language processing [Wang et al., 2015]. [...] Results are shown in Table 2 for our method with compar-isons to related work. The experimental results of these com-pared methods are extracted from their paper. In this case, the unsupervised features are still trained on the whole dataset, but the SVM is trained only on the N labeled points where N varies from 100 to 3K. Also, we compare the performance of Proceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17) 1565 Table 1: Average recognition",
    "url": "https://www.ijcai.org/proceedings/2017/0216.pdf",
    "ext_info": null
  },
  {
    "id": 23,
    "gmt_created": "2025-09-15T01:24:51.255431",
    "gmt_modified": "2025-09-15T01:24:51.255431",
    "task_id": "7373043784122277888",
    "task_no": 23,
    "title": "[PDF] Why Does Unsupervised Pre-training Help Deep Learning?",
    "type": "search_result",
    "content": "To further probe this, the paper plots the online classiﬁcation error (on the next block of examples, as a moving average) for 6 architectures that are trained on InﬁniteMNIST: 1 and 3-layer DBNs, 1 and 3-layer SDAE, as well as 1 and 3-layer networks without pre-training in Figure 11. Here, we can observe that (a) 3-layer networks without pre-training are worse at generalization, compared to the 1-layer equivalents and (b) more importantly, the pre-training advantage does not vanish as the [...] This hopes to capture a bit more complex of a representation than the ﬁrst layer. This method of greedily training each layer one at a time is called pre-training with stacked auto-encoders. Finally all the weights are initialised to the learnt weights and the backpropagation algorithm is run on supervised data to get the ﬁnal weights. This step is called ﬁnetuning. This procedure of learning has been shown to outperform just learning a DNN all at once from randomly initialised weights. There [...] 1- to 4-hidden-layer DNN auto-encoders, with 64 units per hidden layer, were trained for a speciﬁed number of batch gradient descent iterations. Pre-training used stacked auto-encoders (but no denoising).\n20 each layer) than with a kernel representation. Kernels may also be harder to scale up.\nHowever, there is current research into “deep kernels” too.\n• Why does it seem Deep Learning is used mostly for image data?",
    "url": "https://www.stat.cmu.edu/~ryantibs/journalclub/deep.pdf",
    "ext_info": null
  },
  {
    "id": 24,
    "gmt_created": "2025-09-15T01:24:51.262337",
    "gmt_modified": "2025-09-15T01:24:51.262337",
    "task_id": "7373043784122277888",
    "task_no": 24,
    "title": "(PDF) Unsupervised Pre-training of a Deep LSTM-based Stacked ...",
    "type": "search_result",
    "content": "In this paper, we propose a pre-trained LSTM-based stacked autoencoder (LSTM-SAE) approach in an unsupervised learning fashion to replace the random weight",
    "url": "https://www.researchgate.net/publication/337930543_Unsupervised_Pre-training_of_a_Deep_LSTM-based_Stacked_Autoencoder_for_Multivariate_Time_Series_Forecasting_Problems",
    "ext_info": null
  },
  {
    "id": 25,
    "gmt_created": "2025-09-15T01:24:56.752912",
    "gmt_modified": "2025-09-15T01:24:56.752912",
    "task_id": "7373043784122277888",
    "task_no": 25,
    "title": "A Fast Learning Algorithm for Deep Belief Nets - Semantic Scholar",
    "type": "search_result",
    "content": "DOI:10.1162/neco.2006.18.7.1527\n Corpus ID: 2309950\n\n# A Fast Learning Algorithm for Deep Belief Nets\n\n```\n@article{Hinton2006AFL,\n  title={A Fast Learning Algorithm for Deep Belief Nets},\n  author={Geoffrey E. Hinton and Simon Osindero and Yee Whye Teh},\n  journal={Neural Computation},\n  year={2006},\n  volume={18},\n  pages={1527-1554},\n  url={\n}\n```\n\n Geoffrey E. Hinton, Simon Osindero, Y. Teh\n Published in Neural Computation 1 July 2006\n Computer Science [...] These experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy helps the optimization by initializing weights in a region near a good local minimum, but also implicitly acts as a sort of regularization that brings better generalization and encourages internal distributed representations that are high-level abstractions of the input.\n\n 1,221\n PDF\n\n### An Efficient Learning Procedure for Deep Boltzmann Machines\n\nR. SalakhutdinovGeoffrey E. Hinton",
    "url": "https://www.semanticscholar.org/paper/A-Fast-Learning-Algorithm-for-Deep-Belief-Nets-Hinton-Osindero/8978cf7574ceb35f4c3096be768c7547b28a35d0",
    "ext_info": null
  },
  {
    "id": 26,
    "gmt_created": "2025-09-15T01:24:56.761112",
    "gmt_modified": "2025-09-15T01:24:56.761112",
    "task_id": "7373043784122277888",
    "task_no": 26,
    "title": "An Introductory Review of Deep Learning for Prediction Models With ...",
    "type": "search_result",
    "content": ", is widely considered a breakthrough year because in Hinton et al. (2006) it was shown that neural networks called Deep Belief Networks can be efficiently trained by using a strategy called greedy layer-wise pre-training. This initiated the third wave of neural networks that made also the use of the term _deep learning_ popular. [...] Models of artificial neural networks have been used since about the 1950s (Rosenblatt, 1957); however, the current wave of deep learning neural networks started around 2006 (Hinton et al., 2006). A common characteristic of the many variations of supervised and unsupervised deep learning models is that these models have many layers of hidden neurons learned, e.g., by a Restricted Boltzmann Machine (RBM) in combination with Backpropagation and error gradients of the Stochastic Gradient Descent [...] is such a novel methodology currently receiving much attention (Hinton et al., 2006). DL describes a family of learning algorithms rather than a single method that can be used to learn complex prediction models, e.g., multi-layer neural networks with many hidden units (LeCun et al., 2015). Importantly, deep learning has been successfully applied to several application problems. For instance, a deep learning method set the record for the classification of handwritten digits of the MNIST data",
    "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC7861305/",
    "ext_info": null
  },
  {
    "id": 27,
    "gmt_created": "2025-09-15T01:24:56.768507",
    "gmt_modified": "2025-09-15T01:24:56.768507",
    "task_id": "7373043784122277888",
    "task_no": 27,
    "title": "A fast learning algorithm for deep belief nets - PubMed",
    "type": "search_result",
    "content": "1 Department of Computer Science, University of Toronto, Canada. hinton@cs.toronto.edu\n\n    PMID: 16764513\n    DOI: 10.1162/neco.2006.18.7.1527\n\n Item in Clipboard \n\nA fast learning algorithm for deep belief nets\n\nGeoffrey E Hinton et al. Neural Comput.2006 Jul.\n\nShow details\n\nDisplay options\n\n Display options \n\n Format \n\n Neural Comput \n\nActions\n\n   Search in PubMed\n   Search in NLM Catalog\n   Add to Search\n\n. 2006 Jul;18(7):1527-54.\n\n doi: 10.1162/neco.2006.18.7.1527. \n\n### Authors [...] Title & authors\n   Abstract\n   Similar articles\n   Cited by\n   Publication types\n   MeSH terms\n   LinkOut - more resources\n\n Neural Comput \n\nActions\n\n   Search in PubMed\n   Search in NLM Catalog\n   Add to Search\n\n. 2006 Jul;18(7):1527-54.\n\n doi: 10.1162/neco.2006.18.7.1527. \n\nA fast learning algorithm for deep belief nets\n\nGeoffrey E Hinton1,Simon Osindero,Yee-Whye Teh\n\n Affiliations  Expand \n\n### Affiliation [...] Geoffrey E Hinton1,Simon Osindero,Yee-Whye Teh\n\n### Affiliation\n\n   1 Department of Computer Science, University of Toronto, Canada. hinton@cs.toronto.edu\n\n    PMID: 16764513\n    DOI: 10.1162/neco.2006.18.7.1527\n\n Item in Clipboard \n\n Full text links Cite\n\nDisplay options\n\n Display options \n\n Format \n\nAbstract",
    "url": "https://pubmed.ncbi.nlm.nih.gov/16764513/",
    "ext_info": null
  },
  {
    "id": 28,
    "gmt_created": "2025-09-15T01:24:56.775653",
    "gmt_modified": "2025-09-15T01:24:56.775653",
    "task_id": "7373043784122277888",
    "task_no": 28,
    "title": "[PDF] On the Origin of Deep Learning - Uberty",
    "type": "search_result",
    "content": "deep neural networks in practice 1997 Schuster & Paliwal introduced Bidirectional Recurrent Neural Network Hochreiter & Schmidhuber introduced LSTM, solved the problem of vanishing gradient in recurrent neural networks 2006 Geoﬀrey Hinton introduced Deep Belief Networks, also introduced layer-wise pretraining technique, opened current deep learning era. [...] With the establishment of the deep neural network, this paper diverges into three dif-ferent popular deep learning topics. Speciﬁcally, in Section 4, this paper elaborates how Deep Belief Nets and its construction component Restricted Boltzmann Machine evolve as a trade-oﬀof modeling power and computation loads. In Section 5, this paper focuses on the development history of Convolutional Neural Network, featured with the prominent steps along the ladder of ImageNet competition. In Section 6,",
    "url": "https://uberty.org/wp-content/uploads/2017/05/deep-learning-history.pdf",
    "ext_info": null
  },
  {
    "id": 29,
    "gmt_created": "2025-09-15T01:25:02.505906",
    "gmt_modified": "2025-09-15T01:25:02.505906",
    "task_id": "7373043784122277888",
    "task_no": 29,
    "title": "Deep Neural Network - an overview | ScienceDirect Topics",
    "type": "search_result",
    "content": "Deep belief networks (DBNs)(Bengio, 2009) are a type of multi-layer network initially developed by Hinton, Osindero, and Teh (2006). They efficiently use greedy layer-wise unsupervised learning and are made of stochastic binary units, meaning that the binary state of the unit is updated using a probability function. The layer-wise method stacks pre-trained, single-layer learning modules known as restricted Boltzmann machines (RBMs). The representation layer in an RBM is restricted from having [...] Fig. 7. (a): The DBN proposed by Hinton et al. (2006) for MNIST image classification. This network consists of three stacked RBMs with 500, 500, and 2000 representation neurons. The input and output include 784 (as the number of pixels, 28×28) and 10 (as the number of classes, 0, …, 9) neurons, respectively. (b): The spiking RBM architecture introduced by Neftci, Das, Pedroni, Kreutz-Delgado, and Cauwenberghs (2014) consisting of 500 hidden neurons, 784 input neurons, and 40 class neurons (824 [...] Deep Neural Networks (DNNs), also called convolutional networks, are composed of multiple levels of nonlinear operations, such as neural nets with many hidden layers (Bengio et al., 2007; Krizhevsky et al., 2012). Deep learning methods aim at learning feature hierarchies, where features at higher levels of the hierarchy are formed using the features at lower levels (Dean et al., 2012). In 2006, Hinton et al. (2006) proved that much better results could be achieved in deeper architectures when",
    "url": "https://www.sciencedirect.com/topics/mathematics/deep-neural-network",
    "ext_info": null
  },
  {
    "id": 30,
    "gmt_created": "2025-09-15T01:25:02.515910",
    "gmt_modified": "2025-09-15T01:25:02.515910",
    "task_id": "7373043784122277888",
    "task_no": 30,
    "title": "A deep learning framework for financial time series using stacked ...",
    "type": "search_result",
    "content": "Several deep neural network architectures have been proposed in recent studies,including\n\ndeep Boltzmann machines(DBMs),deep belief networks(DBNs)and stacked auto-\n\nencoders(SAEs).Restricted Boltzmann machines(RBMs),convolutional neural net-\n\nworks(CNNs),and autoencodersare the frequently used layer-wise training models.\n\nIn this paper,autoencoders is applied for layer-wise training for the OHLC variables and tech- [...] decomposition of x(t)is the sequence of{S\n\nJ\n\n(t),D\n\nJ\n\n(t),D\n\nJ−1\n\n(t),...D\n\n1\n\n(t)}.When the financial time\n\nseries is very rough,the discrete wavelet transformation can be applied repeatedly by which\n\nthe risk of overfitting can be reduced.As a result,the two-level wavelet is applied twice in this\n\nstudy for data preprocessing as suggested in.\n\nStacked autoencoders\n\nDeep learning is a series of models that have the ability to extract deep features from input data [...] improved the state-of-the-art in image recognition[12,39–41],speech recognition[42–44],\n\nlanguage translation[45,46]and many other areas such as drug discoveryand genomics\n\n[48,49].The main contribution of this work is that it is the first attempt to apply stacked auto-\n\nencoders to generate the deep features of the OHLC,technical indicators and macroeconomic\n\nconditions as a multivariate signal in order to feed to a LSTM to forecast future stock prices.",
    "url": "https://www.researchgate.net/publication/318991900_A_deep_learning_framework_for_financial_time_series_using_stacked_autoencoders_and_long-short_term_memory",
    "ext_info": null
  },
  {
    "id": 31,
    "gmt_created": "2025-09-15T01:25:02.523138",
    "gmt_modified": "2025-09-15T01:25:02.523138",
    "task_id": "7373043784122277888",
    "task_no": 31,
    "title": "[PDF] A State-of-the-Art Survey on Deep Learning Theory and Architectures",
    "type": "search_result",
    "content": "to yield worse results (both in training and in test error) than shallow ones (with 1 or 2 hidden layers). Hinton’s revolutionary work on DBNs spearheaded a change in this in 2006 [56,59]. Due to their composition, many layers of DNNs are more capable of representing highly varying nonlinear functions compared to shallow learning approaches [62–65]. Moreover, DNNs are more efficient for learning because of the combination of feature extraction and classification layers. The following sections [...] GoogLeNet with Inception units [10,70,71], and Residual Networks . The basic building components (convolution and pooling) are almost the same across these architectures. However, some topological differences are observed in the modern deep learning architectures. Of the many DCNN architectures, AlexNet , VGG , GoogLeNet [10,70,71], Dense CNN  and FractalNet  are generally considered the most popular architectures because of their state-of-the-art performance on different benchmarks for object [...] employed in several situations where machine intelligence would be useful (see Figure 3):  Absence of a human expert (navigation on Mars)  Humans are unable to explain their expertise (speech recognition, vision, and language understanding)  The solution to the problem changes over time (tracking, weather prediction, preference, stock, price prediction)  Solutions need to be adapted to the particular cases (biometrics, personalization).  The problem size is too vast for our limited",
    "url": "https://pdfs.semanticscholar.org/d86d/e38c9e184fc296ffc27d585b759ba54ae55c.pdf",
    "ext_info": null
  },
  {
    "id": 32,
    "gmt_created": "2025-09-15T01:25:07.461750",
    "gmt_modified": "2025-09-15T01:25:07.461750",
    "task_id": "7373043784122277888",
    "task_no": 32,
    "title": "Deep Belief Networks: Artificial Intelligence Explained - Netguru",
    "type": "search_result",
    "content": "The connections between the visible and hidden layers in an RBM are symmetrical, meaning that the weight of the connection from a visible unit to a hidden unit is the same as the weight of the connection from the hidden unit to the visible unit. This symmetry allows the RBM to be trained efficiently using a simple learning algorithm called contrastive divergence.\n\n#### Layers in Deep Belief Networks [...] The training process for a Restricted Boltzmann Machine involves adjusting the weights of the connections between the visible and hidden units to improve the ability of the RBM to model the data. This is done using a simple learning algorithm called contrastive divergence. [...] Contrastive divergence involves repeatedly applying two steps: a positive phase and a negative phase. In the positive phase, the RBM is shown a data sample, and the weights are adjusted to make the RBM more likely to produce that sample. In the negative phase, the RBM is allowed to generate a sample from its current model of the data, and the weights are adjusted to make the RBM less likely to produce that sample.\n\n#### Backpropagation in Deep Belief Networks",
    "url": "https://www.netguru.com/glossary/deep-belief-networks",
    "ext_info": null
  },
  {
    "id": 33,
    "gmt_created": "2025-09-15T01:25:07.472558",
    "gmt_modified": "2025-09-15T01:25:07.472558",
    "task_id": "7373043784122277888",
    "task_no": 33,
    "title": "[PDF] Training Restricted Boltzmann Machines: An Introduction⋆",
    "type": "search_result",
    "content": "Algorithm 1: k-step contrastive divergence Input: RBM (V1, . . . , Vm, H1, . . . , Hn), training batch S Output: gradient approximation ∆wij, ∆bj and ∆ci for i = 1, . . . , n, j = 1, . . . , m 1 init ∆wij = ∆bj = ∆ci = 0 for i = 1, . . . , n, j = 1, . . . , m 2 forall the v ∈S do 3 v(0) ←v 4 for t = 0, . . . , k −1 do 5 for i = 1, . . . , n do sample h(t) i ∼p(hi | v(t)) 6 ; 7 for j = 1, . . . , m do sample v(t+1) j ∼p(vj | h(t)) 8 ; 9 for i = 1, . . . , n, j = 1, . . . , m do 10 ∆wij ←∆wij + [...] 5.2 Persistent contrastive divergence The idea of persistent contrastive divergence (PCD)  is described in  for log-likelihood maxi-mization of general MRFs and is applied to RBMs in . The PCD approximation is obtained from the CD approximation (32) by replacing the sample v(k) by a sample from a Gibbs chain that is in-dependent of the sample v(0) of the training distribution. The algorithm corresponds to standard CD learning without reinitializing the visible units of the Markov chain with a [...] 5.1 Contrastive divergence Obtaining unbiased estimates of the log-likelihood gradient using MCMC methods typically requires many sampling steps. However, it has been shown that estimates obtained after running the chain for just a few steps can be suﬃcient for model training . This leads to contrastive divergence (CD) learning, which has become a standard way to train RBMs [21, 4, 24, 3, 23].",
    "url": "https://christian-igel.github.io/paper/TRBMAI.pdf",
    "ext_info": null
  },
  {
    "id": 34,
    "gmt_created": "2025-09-15T01:25:07.481238",
    "gmt_modified": "2025-09-15T01:25:07.481238",
    "task_id": "7373043784122277888",
    "task_no": 34,
    "title": "[PDF] An Efficient Learning Procedure for Deep Boltzmann Machines",
    "type": "search_result",
    "content": "1. Train the first-layer “RBM” using one-step contrastive divergence learning with mean-ﬁeld reconstructions of the visible vectors. During the learning, constrain the bottom-up weights, 2W (1), to be twice the top-down weights, W (1). [...] Welling (2009) has independently reported a closely related phenomenon that he calls “herding.” Recently Tieleman (2008), Salakhutdinov and Hinton (2009a), Salakhut-dinov (2009), and Desjardins, Courville, Bengio, Vincent, and Delalleau (2010) have shown that this stochastic approximation algorithm, also termed persistent contrastive divergence, performs well compared to con-trastive divergence at learning good generative models in RBMs. Although the allowable learning rate is much higher than [...] contrastive divergence is then exactly equivalent to training an RBM with only one hidden layer but with bottom-up weights that are twice the top-down weights, as prescribed An Efﬁcient Learning Procedure for Deep Boltzmann Machines 1987 Algorithm 3: Greedy Pretraining Algorithm for a Deep Boltzmann Machine.",
    "url": "https://www.cs.cmu.edu/~rsalakhu/papers/neco_DBM.pdf",
    "ext_info": null
  },
  {
    "id": 35,
    "gmt_created": "2025-09-15T01:25:48.925108",
    "gmt_modified": "2025-09-15T01:25:48.925108",
    "task_id": "7373043784122277888",
    "task_no": 35,
    "title": "AlexNet - Wikipedia",
    "type": "search_result",
    "content": "The three formed team SuperVision and submitted AlexNet in the ImageNet Large Scale Visual Recognition Challenge on September 30, 2012. The network achieved a top-5 error of 15.3%, more than 10.8 percentage points better than that of the runner-up.\n\nThe architecture influenced a large number of subsequent work in deep learning, especially in applying neural networks \"Neural network (machine learning)\") to computer vision.\n\n## Architecture\n\n[edit] [...] The ImageNet training set contained 1.2 million images. The model was trained for 90 epochs over a period of five to six days using two Nvidia GTX 580 GPUs (3GB each). These GPUs have a theoretical performance of 1.581 TFLOPS in float32 and were priced at US$500 upon release. Each forward pass of AlexNet required approximately 1.43 GFLOPs. Based on these values, the two GPUs together were theoretically capable of performing over 2,200 forward passes per second under ideal conditions. [...] AlexNet is a convolutional neural network architecture developed for image classification tasks, notably achieving prominence through its performance in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). It classifies images into 1,000 distinct object categories and is regarded as the first widely recognized application of deep convolutional networks in large-scale visual recognition.",
    "url": "https://en.wikipedia.org/wiki/AlexNet",
    "ext_info": null
  },
  {
    "id": 36,
    "gmt_created": "2025-09-15T01:25:48.935736",
    "gmt_modified": "2025-09-15T01:25:48.935736",
    "task_id": "7373043784122277888",
    "task_no": 36,
    "title": "[PDF] ImageNet Winning CNN Architectures – A Review",
    "type": "search_result",
    "content": "challenge from 2012 to 2015 in this report. III. AlexNet AlexNet  is considered to be the break-through paper which rose the interest in CNNs when it won the ImageNet challenge of 2012. AlexNet is a deep CNN trained on ImageNet and outperformed all the entries that year. It was a major improvement with the next best entry getting only 26.2% top 5 test error rate. Compared to modern architectures, a relatively simple layout was used in this paper. The architecture from their paper is as follows:",
    "url": "https://rajatvikramsingh.github.io/media/DeepLearning_ImageNetWinners.pdf",
    "ext_info": null
  },
  {
    "id": 37,
    "gmt_created": "2025-09-15T01:25:48.943156",
    "gmt_modified": "2025-09-15T01:25:48.943156",
    "task_id": "7373043784122277888",
    "task_no": 37,
    "title": "AlexNet: Revolutionizing Deep Learning in Image Classification",
    "type": "search_result",
    "content": "The AlexNet architecture dominated in 2012 by achieving a top-5 error rate of 15.3%, significantly lower than the runner-up’s 26.2%.\n\nThis large reduction in error rate excited the researchers with the untapped potential of deep neural networks in handling large image datasets. Subsequently, various Deep Learning models were developed later.\n\n### AlexNet inspired Networks\n\nThe success of AlexNet inspired the design and development of various neural network architectures. These include: [...] In that competition, AlexNet performed exceptionally well. It significantly outperformed the runner-up model by reducing the top-5 error rate from 26.2% to 15.3%.\n\n## Before AlexNet [...] The Deep Learning (DL) model was designed to compete in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. This is an annual competition that benchmarks algorithms for image classification. Before ImageNet, there was no availability of a large dataset to train Deep Neural Networks. As a result, consequently, ImageNet also played a role in the success of Deep Learning.",
    "url": "https://viso.ai/deep-learning/alexnet/",
    "ext_info": null
  },
  {
    "id": 38,
    "gmt_created": "2025-09-15T01:25:48.950239",
    "gmt_modified": "2025-09-15T01:25:48.950239",
    "task_id": "7373043784122277888",
    "task_no": 38,
    "title": "2012: AlexNet Wins the ImageNet Challenge - AskPromotheus.ai",
    "type": "search_result",
    "content": "In 2012, AlexNet achieved atop-5 error rate of 15.3%, outperforming the second-place model by a staggering 10.8 percentage points. This dramatic improvement highlighted the power of deep learning over traditional machine learning methods, which relied heavily on handcrafted features.\n\nWhy Was AlexNet Revolutionary?\n\nAlexNet’s victory marked several key breakthroughs:\n\n1.   Deep Learning’s Viability: [...] The triumph of AlexNet in 2012 reshaped artificial intelligence by demonstrating the power of deep learning in large-scale image classification tasks. Its innovative architecture not only set new benchmarks but also inspired a wave of research that continues to drive advancements in AI today. As industries integrate these technologies into their operations, reflecting on foundational moments like this encourages us to explore both opportunities and challenges responsibly.\n\nSearch [...] In addition to practical applications, AlexNet’s architecture influenced modern AI research. Its principles continue to shape neural network design for tasks like natural language processing (NLP) and generative modeling.\n\nDeep Learning Market Today\n\nThe impact of AlexNet can be seen in the explosive growth of the deep learning market:",
    "url": "https://askpromotheus.ai/artificial-intelligence/history-ai/2012-alexnet-wins-the-imagenet-challenge/",
    "ext_info": null
  },
  {
    "id": 39,
    "gmt_created": "2025-09-15T01:25:54.911436",
    "gmt_modified": "2025-09-15T01:25:54.911436",
    "task_id": "7373043784122277888",
    "task_no": 39,
    "title": "VGG-Net Architecture Explained - Medium",
    "type": "search_result",
    "content": "In ImageNet, the VGG16 model achieves top-5 test accuracy of about 92.7 per cent. A dataset called ImageNet has over 14 million photos that fall into almost 1000 types. It was also among the most well-liked models submitted at ILSVRC-2014. It significantly outperforms AlexNet by substituting several 3x3 kernel-sized filters for the huge kernel-sized filters. Nvidia Titan Black GPUs were used to train the VGG16 model over many weeks.",
    "url": "https://medium.com/@siddheshb008/vgg-net-architecture-explained-71179310050f",
    "ext_info": null
  },
  {
    "id": 40,
    "gmt_created": "2025-09-15T01:25:54.921714",
    "gmt_modified": "2025-09-15T01:25:54.921714",
    "task_id": "7373043784122277888",
    "task_no": 40,
    "title": "VGG Neural Networks: The Next Step After AlexNet - Medium",
    "type": "search_result",
    "content": "Results. On a single test scale, VGG achieved a top-1 error of 25.5% and a top-5 error of 8.0%. At multiple test scales, VGG got a top-1 error of 24.8% and a top-5 error of 7.5%. VGG also achieved second place in the 2014 ImageNet competition with its top-5 error of 7.3%, which they decreased to 6.8% after the submission. [...] Now what? VGG is an innovative object-recognition model that supports up to 19 layers. Built as a deep CNN, VGG also outperforms baselines on many tasks and datasets outside of ImageNet. VGG is now still one of the most used image-recognition architectures.\n\nI’ve attached some further resources below that may be interesting\n\nSome rights reserved\n\n--\n\n--\n\n1\n\nTDS Archive\nTDS Archive\n\n## Published in TDS Archive [...] VGG Neural Networks. While previous derivatives of AlexNet focused on smaller window sizes and strides in the first convolutional layer, VGG addresses another very important aspect of CNNs: depth. Let’s go over the architecture of VGG:\n\nThe Difference. VGG, while based off of AlexNet, has several differences that separates it from other competing models:",
    "url": "https://medium.com/data-science/vgg-neural-networks-the-next-step-after-alexnet-3f91fa9ffe2c",
    "ext_info": null
  },
  {
    "id": 41,
    "gmt_created": "2025-09-15T01:25:54.929462",
    "gmt_modified": "2025-09-15T01:25:54.929462",
    "task_id": "7373043784122277888",
    "task_no": 41,
    "title": "CNN Architectures: LeNet, AlexNet, VGG, GoogLeNet, ResNet and ...",
    "type": "search_result",
    "content": "The runner-up at the ILSVRC 2014 competition is dubbed VGGNet by the community and was developed by Simonyan and Zisserman. VGGNet consists of 16 convolutional layers and is very appealing because of its very uniform architecture. Similar to AlexNet, only 3x3 convolutions, but lots of filters. Trained on 4 GPUs for 2–3 weeks. It is currently the most preferred choice in the community for extracting features from images. The weight configuration of the VGGNet is publicly available and has been [...] ## VGGNet (2014)",
    "url": "https://medium.com/analytics-vidhya/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5",
    "ext_info": null
  },
  {
    "id": 42,
    "gmt_created": "2025-09-15T01:25:54.937556",
    "gmt_modified": "2025-09-15T01:25:54.937556",
    "task_id": "7373043784122277888",
    "task_no": 42,
    "title": "A Review of Popular Deep Learning Architectures: AlexNet, VGG16 ...",
    "type": "search_result",
    "content": "VGG is a popular neural network architecture proposed by Karen Simonyan & Andrew Zisserman from the University of Oxford. It is also based on CNNs, and was applied to the ImageNet Challenge in 2014. The authors detail their work in their paper, Very Deep Convolutional Networks for large-scale Image Recognition. The network achieved 92.7% top-5 test accuracy on the ImageNet dataset. [...] Major improvements of VGG, when compared to AlexNet, include using large kernel-sized filters (sizes 11 and 5 in the first and second convolutional layers, respectively) with multiple (3×3) kernel-sized filters, one after another.\n\n## VGG Architecture\n\nThe input dimensions of the architecture are fixed to the image size, (244 × 244). In a pre-processing step the mean RGB value is subtracted from each pixel in an image.\n\nSource: Step by step VGG16 implementation in Keras for beginners [...] 1. Tensorflow AlexNet Model\n2. Other references: Understanding AlexNet\n3. The original paper: ImageNet Classification with Deep Convolutional Neural Networks\n\n## VGG16 (2014)",
    "url": "https://www.digitalocean.com/community/tutorials/popular-deep-learning-architectures-alexnet-vgg-googlenet",
    "ext_info": null
  },
  {
    "id": 43,
    "gmt_created": "2025-09-15T01:25:54.945404",
    "gmt_modified": "2025-09-15T01:25:54.945404",
    "task_id": "7373043784122277888",
    "task_no": 43,
    "title": "VGG vs. AlexNet: A Comparative Study in Convolutional Neural ...",
    "type": "search_result",
    "content": "Architecture: VGG explored the impact of network depth on accuracy. Its architectures ranged from 11 to 19 weight layers, consistently using very small 3x3 convolution filters throughout the entire network, with a stride of 1 pixel. Spatial pooling was handled by five max-pooling layers with a 2x2 pixel window and stride 2. Similar to AlexNet, it also concluded with three fully-connected layers (two with 4096 channels, one 1000-way for ILSVRC classification) and a softmax layer. [...] The landscape of deep learning for image recognition has been significantly shaped by two pioneering architectures: AlexNet and VGG. Both models made monumental strides in the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC), pushing the boundaries of what convolutional neural networks (CNNs) could achieve. This article delves into a comparative study of these influential models, highlighting their key characteristics and contributions.\n\nAlexNet: The Breakthrough (ILSVRC 2012 Winner) [...] The VGG network, developed by Karen Simonyan and Andrew Zisserman from the Visual Geometry Group at the University of Oxford, was a prominent entry in the ILSVRC 2014. Their key contribution was a thorough evaluation of networks of increasing depth using very small convolutional filters.",
    "url": "https://www.linkedin.com/pulse/vgg-vs-alexnet-comparative-study-convolutional-neural-pankaj-somkuwar-tiwve",
    "ext_info": null
  },
  {
    "id": 44,
    "gmt_created": "2025-09-15T01:26:00.102578",
    "gmt_modified": "2025-09-15T01:26:00.102578",
    "task_id": "7373043784122277888",
    "task_no": 44,
    "title": "ILSVRC2014 Results - ImageNet",
    "type": "search_result",
    "content": "1) We used the additional training annotations for the 2014 detection dataset. 2) We used a slightly larger convolutional neural network than in [1, 2]. In this",
    "url": "https://image-net.org/challenges/LSVRC/2014/results",
    "ext_info": null
  },
  {
    "id": 45,
    "gmt_created": "2025-09-15T01:26:00.114367",
    "gmt_modified": "2025-09-15T01:26:00.114367",
    "task_id": "7373043784122277888",
    "task_no": 45,
    "title": "Inception (deep learning architecture) - Wikipedia",
    "type": "search_result",
    "content": "**Inception** is a family of convolutional neural network (CNN) for computer vision, introduced by researchers at Google in 2014 as GoogLeNet (later renamed Inception v1). Because later, more versions were released, the original Inception architecture was renamed again as \"Inception v1\". The Inception v1 architecture is a deep CNN composed of 22 layers. It improves on Inception v2 by using factorized convolutions. | Machine learning | |  |  | | --- | --- | | Neural networks | * Inception (2014) * WaveNet (2016) * MobileNet (2017) * Transformer \"Transformer (deep learning architecture)\") (2017) * EfficientNet (2019) * Gato \"Gato (DeepMind)\") (2022) | | Other | * Quantum Artificial Intelligence Lab * TensorFlow * Tensor Processing Unit | |",
    "url": "https://en.wikipedia.org/wiki/Inception_(deep_learning_architecture)",
    "ext_info": null
  },
  {
    "id": 46,
    "gmt_created": "2025-09-15T01:26:00.127261",
    "gmt_modified": "2025-09-15T01:26:00.127261",
    "task_id": "7373043784122277888",
    "task_no": 46,
    "title": "[1409.4842] Going Deeper with Convolutions - arXiv",
    "type": "search_result",
    "content": "**arXiv:1409.4842** (cs)  Authors:Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich View a PDF of the paper titled Going Deeper with Convolutions, by Christian Szegedy and 7 other authors View a PDF of the paper titled Going Deeper with Convolutions, by Christian Szegedy and 7 other authors - [x] Bibliographic Explorer Toggle  - [x] Connected Papers Toggle  - [x] Litmaps Toggle  - [x] alphaXiv Toggle  - [x] Links to Code Toggle  - [x] DagsHub Toggle  - [x] GotitPub Toggle  - [x] Huggingface Toggle  - [x] Links to Code Toggle  - [x] ScienceCast Toggle  - [x] Replicate Toggle  - [x] Spaces Toggle  - [x] Spaces Toggle  - [x] Core recommender toggle ",
    "url": "https://arxiv.org/abs/1409.4842",
    "ext_info": null
  },
  {
    "id": 47,
    "gmt_created": "2025-09-15T01:26:00.138147",
    "gmt_modified": "2025-09-15T01:26:00.138147",
    "task_id": "7373043784122277888",
    "task_no": 47,
    "title": "2014: ImageNet Large Scale Visual Recognition Challenge",
    "type": "search_result",
    "content": "This challenge evaluates algorithms for object detection and image classification at large scale. 1. A PASCAL-style detection challenge on fully labeled data for 200 categories of objects, and 2. An image classification plus object localization challenge with 1000 categories. **NEW:** This year all participants are encouraged to submit object localization results; in past challenges, submissions to classification and classification with localization tasks were accepted separately. These images were fully annotated with the 200 object categories, yielding 132953 new For each image, algorithms will produce a set of annotations (ci,bi,si) of class labels Entires submitted to ILSVRC2014 will be divided into two tracks: \"provided data\" track (entries using only ILSVRC2014 images and annotations provided for each task), and \"external data\" track (entries using any outside images or annotations).",
    "url": "https://www.image-net.org/challenges/LSVRC/2014/",
    "ext_info": null
  },
  {
    "id": 48,
    "gmt_created": "2025-09-15T01:26:00.147194",
    "gmt_modified": "2025-09-15T01:26:00.147194",
    "task_id": "7373043784122277888",
    "task_no": 48,
    "title": "Review: GoogLeNet (Inception v1)— Winner of ILSVRC 2014 ...",
    "type": "search_result",
    "content": "In this story, **GoogLeNet [1]** is reviewed, which is the winner of the **ILSVRC (****ImageNet Large Scale Visual Recognition Competition****) 2014**, an image classification competition, which has significant improvement over ZFNet (The winner in 2013) [2] and AlexNet (The winner in 2012) [3], and has relatively lower error rate compared with the VGGNet (1st runner-up in 2014) [4]. The network architecture in this paper is quite different from VGGNet, ZFNet, and AlexNet. It contains **1×1 Convolution** at the middle of the network. Thus, **inception module can be built without increasing the number of operations** **largely compared the one without 1×1 convolution!** Thus, 1×1 convolution is inserted into the inception module for dimension reduction! Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "url": "https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7",
    "ext_info": null
  },
  {
    "id": 49,
    "gmt_created": "2025-09-15T01:26:00.157400",
    "gmt_modified": "2025-09-15T01:26:00.157400",
    "task_id": "7373043784122277888",
    "task_no": 49,
    "title": "Microsoft Researchers' Algorithm Sets ImageNet Challenge Milestone",
    "type": "search_result",
    "content": "The race’s new leader is a team of Microsoft researchers in Beijing, which this week published a paper in which they noted their computer vision system based on deep convolutional neural networks (opens in new tab) (CNNs) had for the first time eclipsed the abilities of people to classify objects defined in the ImageNet 1000 challenge. In their paper, *Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification (opens in new tab)*, the researchers say their system achieved a 4.94 percent error rate on the 1000-class ImageNet 2012 classification dataset, which contains about 1.2 million training images, 50,000 validation images, and 100,000 test images. Most recently, Baidu researchers (opens in new tab) have published a paper in which they claim to have achieved “a top-5 error rate of 5.33%.” against the ImageNet classification challenge.",
    "url": "https://www.microsoft.com/en-us/research/blog/microsoft-researchers-algorithm-sets-imagenet-challenge-milestone/",
    "ext_info": null
  },
  {
    "id": 50,
    "gmt_created": "2025-09-15T01:26:05.265890",
    "gmt_modified": "2025-09-15T01:26:05.265890",
    "task_id": "7373043784122277888",
    "task_no": 50,
    "title": "Review: VGGNet — 1st Runner-Up (Image Classification), Winner ...",
    "type": "search_result",
    "content": "Compared with GoogLeNet using 7-nets which has error rate of 6.7%, VGGNet using 2-nets, plus multi scale training, multi-scale testing, mutli-crop and dense has error rate of 6.8% which are competitive.\n\nWith only 1-net, VGGNet has 7.0% error rate which is better than GoogLeNet, that has 7.9% error rate.\n\nHowever, at the submission of ILSVRC 2014, VGGNet has 7.3% error rate only which got 1st runner up at the moment.\n\n# 8. Localization Task (Post Updated on 2nd Sept 2018) [...] In this story, VGGNet  is reviewed. VGGNet is invented by VGG (Visual Geometry Group) from University of Oxford, Though VGGNet is the 1st runner-up, not the winner of the ILSVRC (ImageNet Large Scale Visual Recognition Competition) 2014 in the classification task, which has significantly improvement over ZFNet (The winner in 2013)  and AlexNet (The winner in 2012) . And GoogLeNet is the winner of ILSVLC 2014, I will also talk about it later.) Nevertheless, VGGNet beats the GoogLeNet and won the [...] With multiple training and testing which just described in previous sections, the top-5 localization error is reduced to 25.3%.\n\nVGGNet even outperforms GoogLeNet as shown above and won the localization task in ILSVRC 2014.\n\nVGGNet has the best results on VOC 2007, 2012 and Caltech 256 dataset. And it also has competitive result on Caltech 101 dataset.\n\nI will cover GoogLeNet , ResNet , and so on for the image classification.   \nPlease stay tuned.\n\n# References",
    "url": "https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11",
    "ext_info": null
  },
  {
    "id": 51,
    "gmt_created": "2025-09-15T01:26:05.276556",
    "gmt_modified": "2025-09-15T01:26:05.276556",
    "task_id": "7373043784122277888",
    "task_no": 51,
    "title": "VGG-16 | CNN model - GeeksforGeeks",
    "type": "search_result",
    "content": "Results: VGG-16 was one of the best performing architectures in the ILSVRC challenge 2014.It was the runner up in the classification task with a top-5 classification error of 7.32% (only behind GoogLeNet with a classification error of 6.66%). It was also the winner of localization task with 25.32% localization error.\n\n### Limitations Of VGG 16:",
    "url": "https://www.geeksforgeeks.org/computer-vision/vgg-16-cnn-model/",
    "ext_info": null
  },
  {
    "id": 52,
    "gmt_created": "2025-09-15T01:26:05.284196",
    "gmt_modified": "2025-09-15T01:26:05.284196",
    "task_id": "7373043784122277888",
    "task_no": 52,
    "title": "Imagenet Challenge - an overview | ScienceDirect Topics",
    "type": "search_result",
    "content": "To categorize an image with a size of 224 × 224, it took 138 million weights and 15.5G MAC . GoogLeNet provides an inception module consisting of various sized filters to improve accuracy while decreasing DNN inference computation. GoogLeNet outperforms VGG-16 in terms of accuracy while using just 7 million weights and 1.43G MAC to analyze a picture of the same size. The “shortcut” structure is utilized to attain human-level accuracy with a top-five error rate of less than 5% in the [...] the winner of ILSVRC competition, marked the first time that the convolutional neural network is able to classify with a top-5 error rate of 15.4%, while the top-5 error rate of the second-ranked model is 26.2%. The whole community of computer vision was shocked by the performance and deep learning became popular since then. GoogleNet and VGGNet in 2014 and ResNet in 2015 are subsequent impressive models because of their great achievements. Since then, “model trained on ImageNet” means that the [...] the winner of ILSVRC competition, marked the first time that the convolutional neural network is able to classify with a top-5 error rate of 15.4%, while the top-5 error rate of the second-ranked model is 26.2%. The whole community of computer vision was shocked by the performance and deep learning became popular since then. GoogleNet and VGGNet in 2014 and ResNet in 2015 are subsequent impressive models because of their great achievements. Since then, “model trained on ImageNet” means that the",
    "url": "https://www.sciencedirect.com/topics/computer-science/imagenet-challenge",
    "ext_info": null
  },
  {
    "id": 53,
    "gmt_created": "2025-09-15T01:26:11.436071",
    "gmt_modified": "2025-09-15T01:26:11.436071",
    "task_id": "7373043784122277888",
    "task_no": 53,
    "title": "GoogLeNet: Revolutionizing Deep Learning with Inception - Viso Suite",
    "type": "search_result",
    "content": "The turning point came in 2012 with the introduction of AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. AlexNet, for the ImageNet challenge, significantly outperformed other machine learning approaches. This broughtdeep learning to the forefront of AI research. AlexNet featured several innovations, such as ReLU, dropout for regularization, and overlapping pooling. [...] After AlexNet, researchers started developing complex and deeper networks. GoogLeNet had 22 layers and VGGNet had 16 layers compared to AlexNet, which had only 8 layers in total.\nHowever, in the VGGNet paper, the limitations of simply stacking more layers were highlighted, as it was computationally expensive and led to overfitting. It wasn’t possible to keep increasing the layers without any innovation to cater to these problems.\nArchitecture of GoogLeNet [...] | GoogLeNet | 2014 | 1st | 6.67% | no |\nTable source.\n### Comparison with Other Architectures\n\\ \\\\AlexNet (2012):\\\\ Top-5 Error Rate of 15.3%. The Architecture Consists of 8 layers (5 convolutional and 3 fully connected layers), which used ReLU activations, dropout, and data augmentation to achieve state-of-the-art performance in 2012.\nImage 4: image showing alexNet\nAlexNet Architecture –source",
    "url": "https://viso.ai/deep-learning/googlenet-explained-the-inception-model-that-won-imagenet/",
    "ext_info": null
  },
  {
    "id": 54,
    "gmt_created": "2025-09-15T01:26:11.446935",
    "gmt_modified": "2025-09-15T01:26:11.446935",
    "task_id": "7373043784122277888",
    "task_no": 54,
    "title": "Comparing deep learning architectures for different computer vision ...",
    "type": "search_result",
    "content": "and the aforementioned changes to the architecture: ReLU for faster training and dropout for improved regularization. AlexNet’s impact is undeniable, as evidenced by its citation in over 120,000 papers on Google Scholar. [...] 2012 marked the breakout moment for CNNs with the introduction of AlexNet. It significantly outperformed its competition, achieving an error rate that was 10.8 percentage points lower than the next runner-up in the ImageNet Large Scale Visual Recognition Challenge. One of the primary takeaways from AlexNet’s performance was that deeper networks tended to yield higher performance. The computational demands of AlexNet were made feasible primarily because of GPUs. The network improved upon LeNeT [...] While VGG still maintains fully connected layers that NiN addresses, it introduces a solution to a different key limitation of AlexNet: CNN based networks with stacked blocks of Conv > Relu > Max Pooling can only be as deep given the second logarithm of the number of dimensions in the input. The authors introduced the VGG Block, which replaces single convolutional steps with multiple convolutional steps. For example, two stacked 3x3 convolutions followed by max pooling produces a similar",
    "url": "https://developmentseed.org/tensorflow-eo-training-2/docs/Lesson7a_comparing_architectures.html",
    "ext_info": null
  },
  {
    "id": 55,
    "gmt_created": "2025-09-15T01:26:11.455471",
    "gmt_modified": "2025-09-15T01:26:11.455471",
    "task_id": "7373043784122277888",
    "task_no": 55,
    "title": "Alexnet - an overview | ScienceDirect Topics",
    "type": "search_result",
    "content": "When implementing these CNN’s architectures, we note in traditional algorithms, for example, AlexNet (Krizhevsky et al., 2012), VGG (Simonyan and Zisserman, 2014), ResNet (He et al., 2016), LeNet (Lecun et al., 1998), InceptionV3 (Szegedy et al., 2015b), GoogLeNet (Szegedy et al., 2015a), are predominant in the selected studies. The recurring use of these implementations is explained by their success cases based on the ImageNet challenge (Russakovsky et al., 2014). Approximately 85% of the [...] (Szegedy et al., 2015) and AlexNet, was applied to publicly available plant disease database “PlantVillage” (Hughes and Salathé, 2015) having a public dataset of 54,306 images of diseased and healthy plant leaves collected under controlled conditions. A total of 14 crop species and 26 diseases were classified (Mohanty et al., 2016) using both the CNN architecture. It was found that GoogLeNet architecture consistently outperformed the AlexNet and produced the top accuracy of up to 99.3%. It was [...] a diverse set of images to train the CNN. Ferentinos, 2018, applied five different CNN architectures including AlexNet, GoogLeNet, AlexNetOWTBn (Krizhevsky, 2014), Over feat (Sermanet et al., 2013), and VGG (Simonyan and Zisserman, 2014) on a bigger database by taking 87,848 images, containing different plants (Apple, Banana, Blueberry, Cabbage, Cantaloupe, Cassava, Celery, Cherry, Corn, Cucumber, Eggplant, Gourd, Grape, Onion, Orange, Peach, Pepper, Potato, Pumpkin, Raspberry, Soybean, Squash,",
    "url": "https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/alexnet",
    "ext_info": null
  },
  {
    "id": 56,
    "gmt_created": "2025-09-15T01:27:15.799565",
    "gmt_modified": "2025-09-15T01:27:15.799565",
    "task_id": "7373043784122277888",
    "task_no": 56,
    "title": "ChatGPT version history: Evolution timeline - nexos.ai",
    "type": "search_result",
    "content": "| Model | Release year | Approx. parameter count | MMLU (%) | GSM8K (%) | HumanEval pass@1 (%) |\n ---  ---  --- |\n| GPT-1 | 2018 | 117 million  - \n| GPT-2 | 2019 | 1.5 billion  - \n| GPT-3 | 2020 | 175 billion | 43.9 | 10.4 | 0.0 |\n| GPT-3.5 Turbo | 2023 | ~175 billion | 70.0 | 57.1 | 61.5–67.0 |\n| GPT-4 | 2023 | ~1–1.8 trillion\\ | 86.4 | 92.0 | 67.0 |\n| GPT-4 Turbo | 2024 | ~1–1.8 trillion\\ | 86.7 | 89.6 | 88.2 |\n| GPT-4o | 2024 | ~1–1.8 trillion\\ | 87.2 | 89.9 | 91.0 | [...] In April 2024, OpenAI released GPT-4 Turbo, a specialized variant of GPT-4 optimized for speed, cost-effectiveness, and operational efficiency. Rather than simply scaling parameters, GPT-4 Turbo demonstrated that intelligent design tweaks and targeted fine-tuning could preserve or even enhance performance metrics while reducing computational overhead. [...] | GPT-4o (2024-05-13) | 87.2 | 89.9 | 91.0 | 1304 | 74.8 |\n| GPT-4o (2024-08-06) | 88.7 | 90.0 | 90.2 | 1280 \n| GPT-4.1 (2025-04-14) | 90.2 | 86.9 | 94.5 | 1380 | 80.6 |\n| GPT-4.5 Preview (2025-02-27) | 90.8 | 86.9 | 88.6 | 1418 | 81.0 |\n| ChatGPT-4o (2025-03-26)  -  1430 | 80.3 |\n| ChatGPT-5 | 86.0 | 97  1455 | 87 |",
    "url": "https://nexos.ai/blog/chatgpt-version-history/",
    "ext_info": null
  },
  {
    "id": 57,
    "gmt_created": "2025-09-15T01:27:15.811221",
    "gmt_modified": "2025-09-15T01:27:15.811221",
    "task_id": "7373043784122277888",
    "task_no": 57,
    "title": "The Evolution of Language Models: From GPT-1 to GPT-4 and Beyond",
    "type": "search_result",
    "content": "Introduction: GPT-4, released in 2023, continued the trend of scaling up and enhancing the capabilities of language models. It represented the latest advancement in the GPT series, incorporating improvements in both architecture and training methodologies.\n Architecture: GPT-4 features a model with 500 billion parameters and advanced transformer layers. It was trained on an even more diverse and extensive dataset, further improving its performance and generalization.\n\n### Key Contributions [...] ## GPT-2: Scaling Up\n\n### Overview and Architecture\n\n Introduction: GPT-2, released in 2019, represented a major leap forward in language modeling. It scaled up the architecture of GPT-1 significantly, incorporating more parameters and a larger dataset.\n Architecture: GPT-2 featured 1.5 billion parameters and 48 transformer layers, making it a substantial upgrade from GPT-1. The model was trained on a diverse dataset containing a wide range of internet text.\n\n### Key Contributions [...] ## GPT-3: The Breakthrough\n\n### Overview and Architecture\n\n Introduction: GPT-3, unveiled in 2020, was a landmark development in the field of NLP. It represented a significant increase in model size and capability compared to GPT-2.\n Architecture: GPT-3 consists of 175 billion parameters and 96 transformer layers. It was trained on an even larger and more diverse dataset, including books, articles, and other text sources.\n\n### Key Contributions",
    "url": "https://www.geeksforgeeks.org/artificial-intelligence/the-evolution-of-language-models-from-gpt-1-to-gpt-4-and-beyond/",
    "ext_info": null
  },
  {
    "id": 58,
    "gmt_created": "2025-09-15T01:27:15.819656",
    "gmt_modified": "2025-09-15T01:27:15.819656",
    "task_id": "7373043784122277888",
    "task_no": 58,
    "title": "Timeline of AI and language models - LifeArchitect.ai",
    "type": "search_result",
    "content": "Next in 2024\n\nImage 4\n\nImage 5\n\nImage 6\n\nDownload source (PDF)\n\n  \n\nTime between releases of OpenAI’s GPT models\n\n| Model | Months since last release |\n --- |\n| GPT-4 _Mar/2023_ | 14m | 14 months |\n| GPT-3 2022 text-davinci-002 _Jan/2022_ | 20m | 20 months |\n| GPT-3 _May/2020_ | 15m | 15 months |\n| GPT-2 _Feb/2019_ | 8m | 8 months |\n| GPT-1 _Jun/2018_ | Baseline |  |\n\n  \n\nGet _The Memo_\n\nby Dr Alan D. Thompson ·Be inside the lightning-fast AI revolution. [...] [bold_timeline_item_button title=”Expand” style=”” shape=”” color=”” size=”inline” url=”#” el_class=”bold_timeline_group_button”]\n\n### 2023\n\nFebruary\n\nLLaMA-65B (model)\n\nMeta AI\n\nRead the paper\n\nMarch\n\nAlpaca 7B (model)\n\nStanford\n\nRead the release\n\nMarch\n\nGPT-4 1.76T (model)\n\nOpenAI\n\nRead the paper,\n\nAlan’s analysis\n\nMay\n\nPaLM 2 340B (model)\n\nGoogle\n\nRead the paper\n\nJune\n\nphi-1 1.3B (model)\n\nMicrosoft\n\nRead the paper\n\nJune\n\nInflection-1 (model)\n\nInflection AI\n\nRead the paper\n\nJuly [...] GPT-4.5 (2025)\")\n       GPT-5 (2025)\")\n       GPT-6 (2026)\")\n       GPT-7 (2026)\")\n       Grok\n       Megatron\n       Microsoft Bing Chat (Sydney/GPT-4)\")\n       NVIDIA Cosmos video dataset\n       o1: Smarter than we think (2024)\")\n       o3: Stratospheric reasoning\n       o4: Critical mass\n       o5: Proto-ASI",
    "url": "https://lifearchitect.ai/timeline/",
    "ext_info": null
  },
  {
    "id": 59,
    "gmt_created": "2025-09-15T01:27:20.907233",
    "gmt_modified": "2025-09-15T01:27:20.907233",
    "task_id": "7373043784122277888",
    "task_no": 59,
    "title": "What is PaLM 2?: A Definitive Guide - Simform",
    "type": "search_result",
    "content": "|  |  |  |\n --- \n| Parameter | PaLM 2 | PaLM |\n| Data used for training | 3.6 trillion tokens (size of the parameters is unknown) | 780 billion tokens and 540 billion parameters |\n| Compute-model size | Smaller | Larger |\n| Applications | Multiple applications, including 25 products of Google. | Limited applications |\n| Multilingualism | Support for 100+ languages. | Support for fewer languages | [...] |  |  |  |\n --- \n| Feature | PaLM 2 | GPT-4 |\n| Model size | Unknown | 175 billion parameters |\n| Training data | 560 trillion words | 500 trillion words |\n| Performance | Superior at answering questions comprehensively | Capable of providing accurate answers but not as well as PaLM 2 |\n| Internet access | Connected with Google, which means access to all current events | Connected with Bing AI, which has limited access to current data | [...] PaLM 2 is a large language model developed by Google that is based on the Transformer architecture and the Pathways system. It is trained on a massive dataset of text and code, which gives it a vast knowledge base and the ability to understand and respond to natural language questions, generate text, code, and other creative content, and translate languages. With PaLM 2, Google aims to bring AI capabilities to its products, including Gmail, Google Docs, and Bard.",
    "url": "https://www.simform.com/blog/palm-2/",
    "ext_info": null
  },
  {
    "id": 60,
    "gmt_created": "2025-09-15T01:27:20.918255",
    "gmt_modified": "2025-09-15T01:27:20.918255",
    "task_id": "7373043784122277888",
    "task_no": 60,
    "title": "Pathways Language Model (PaLM): Scaling to 540 Billion ...",
    "type": "search_result",
    "content": "PaLM demonstrates the first large-scale use of the Pathways system to scale training to 6144 chips, the largest TPU-based system configuration used for training to date. The training is scaled using data parallelism at the Pod level across two Cloud TPU v4 Pods, while using standard data and model parallelism within each Pod. This is a significant increase in scale compared to most previous LLMs, which were either trained on a single TPU v3 Pod (e.g., GLaM, LaMDA), used pipeline parallelism to [...] PaLM paves the way for even more capable models by combining the scaling capabilities with novel architectural choices and training schemes, and brings us closer to the Pathways vision:\n\n  \n\n|  |\n\n| “Enable a single AI system to generalize across thousands or millions of tasks, to understand different types of data, and to do so with remarkable efficiency.\" |\n\n  \n\n## Acknowledgements [...] ## Conclusion and Future Work\n\nPaLM demonstrates the scaling capability of the Pathways system to thousands of accelerator chips across two TPU v4 Pods by training a 540-billion parameter model efficiently with a well-studied, well-established recipe of a dense decoder-only Transformer model. Pushing the limits of model scale enables breakthrough few-shot performance of PaLM across a variety of natural language processing, reasoning, and code tasks.",
    "url": "https://research.google/blog/pathways-language-model-palm-scaling-to-540-billion-parameters-for-breakthrough-performance/",
    "ext_info": null
  },
  {
    "id": 61,
    "gmt_created": "2025-09-15T01:27:20.926885",
    "gmt_modified": "2025-09-15T01:27:20.926885",
    "task_id": "7373043784122277888",
    "task_no": 61,
    "title": "What is PaLM 2: Google's Large Language Model Explained",
    "type": "search_result",
    "content": "Palm 2 (Pathways Language Model 2) is one of the most advanced large language models presented by Google and is aimed at natural language understanding as well as generation. It is the next-generation model of the PaLM model and is based on Google’s Pathways system where a single model can handle a variety of tasks efficiently and scale up accordingly. PaLM 2 is trained on vast data which makes it able to grasp the context, synthesise text and even solve some logical problems. It uses so-called [...] Multilingual Capabilities: PaLM 2 has been developed to work in different languages thus it is capable of performing various multilingual operations.\n Advanced Reasoning: It can also carry out major reasoning work such as solving mathematical problems and deducing logical consequences with a good level of precision.\n Efficient Scaling: Due to the Pathways architecture, PaLM 2 can be efficiently scaled across different tasks maintaining the performance and resource consumption. [...] Dependence on Data Quality: A lot of empirical evidence and user feedback shows that the proficiency of PaLM 2 is proportional to the quality and variety of the training data set. This is because the mere outputs of a model may be imperfect or missing like the data used to feed the model.",
    "url": "https://www.geeksforgeeks.org/artificial-intelligence/what-is-palm-2-googles-large-language-model-explained/",
    "ext_info": null
  },
  {
    "id": 62,
    "gmt_created": "2025-09-15T01:27:25.856717",
    "gmt_modified": "2025-09-15T01:27:25.856717",
    "task_id": "7373043784122277888",
    "task_no": 62,
    "title": "Teach Llamas to Talk: Recent Progress in Instruction Tuning",
    "type": "search_result",
    "content": "The emergence of open-source instruction-tuning data, algorithms, and models is one of the most exciting progress for LLMs in 2023. It gives researchers and developers the chance to train, evaluate, interact, and analyze instruction models with full control (from parameters to data), which only existed as black boxes before. The past few months are also a bit “chaotic” for the field, as hundreds of papers released results with different data, algorithms, base models, and even evaluation, making [...] Corresponding to the first purpose, there is multi-task instruction tuning data, which have been heavily explored between 2020-2022. Those data combine thousands of NLP tasks together and give each task a natural language instruction, then one can train models on the combination in a multi-task way (for a more thorough review, see Sebastian Ruder’s great blogpost). Representative datasets include Natural Instruction (Mishra et al., 2021; Wang et al., 2022), T0 (San et al., 2021), and Flan (Wei [...] Now let’s talk about open-ended instruction tuning data, which notably emerged in 2023 (in the following, “SFT data” refer to open-ended instruction tuning data). It is general belief that training with these data does not improve LLMs’ “knowledge” (reflected by scores on traditional benchmarks), but merely “guides” them to follow the instruction-following or conversational format, gaining an engaging tone, being polite, etc (superficial alignment hypothesis; Zhou et al., 2023).",
    "url": "https://gaotianyu.xyz/blog/2023/11/30/instruction-tuning/",
    "ext_info": null
  },
  {
    "id": 63,
    "gmt_created": "2025-09-15T01:27:25.868839",
    "gmt_modified": "2025-09-15T01:27:25.868839",
    "task_id": "7373043784122277888",
    "task_no": 63,
    "title": "Llama (language model) - Wikipedia",
    "type": "search_result",
    "content": "On April 18, 2024, Meta released Llama 3 with two sizes: 8B and 70B parameters. The models have been pre-trained on approximately 15 trillion tokens of text gathered from “publicly available sources” with the instruct models fine-tuned on “publicly available instruction datasets, as well as over 10M human-annotated examples\". Meta AI's testing showed in April 2024 that Llama 3 70B was beating Gemini \"Gemini (chatbot)\") Pro 1.5 and Claude \"Claude (language model)\") 3 Sonnet on most benchmarks. [...] Llama (Large Language Model Meta AI)( is a family of large language models (LLMs) released by Meta AI starting in February 2023.( The latest version is Llama 4, released in April 2025.(\n\nLlama models come in different sizes, ranging from 1 billion to 2 trillion parameters. Initially only a foundation model,( starting with Llama 2, Meta AI released instruction fine-tuned \"Fine-tuning (deep learning)\") versions alongside foundation models.( [...] The Stanford University Institute for Human-Centered Artificial Intelligence (HAI) Center for Research on Foundation Models (CRFM) released Alpaca, a training recipe based on the Llama 7B model that uses the \"Self-Instruct\" method of instruction tuning to acquire capabilities comparable to the OpenAI GPT-3 series text-davinci-003 model at a modest cost.( The model files were officially removed on March 21, 2023, over hosting costs and safety concerns, though the code and paper remain online for",
    "url": "https://en.wikipedia.org/wiki/Llama_(language_model)",
    "ext_info": null
  },
  {
    "id": 64,
    "gmt_created": "2025-09-15T01:27:25.878937",
    "gmt_modified": "2025-09-15T01:27:25.878937",
    "task_id": "7373043784122277888",
    "task_no": 64,
    "title": "Introducing Meta Llama 3: The most capable openly available LLM ...",
    "type": "search_result",
    "content": "Our new 8B and 70B parameter Llama 3 models are a major leap over Llama 2 and establish a new state-of-the-art for LLM models at those scales. Thanks to improvements in pretraining and post-training, our pretrained and instruction-fine-tuned models are the best models existing today at the 8B and 70B parameter scale. Improvements in our post-training procedures substantially reduced false refusal rates, improved alignment, and increased diversity in model responses. We also saw greatly improved [...] Today, we’re excited to share the first two models of the next generation of Llama, Meta Llama 3, available for broad use. This release features pretrained and instruction-fine-tuned language models with 8B and 70B parameters that can support a broad range of use cases. This next generation of Llama demonstrates state-of-the-art performance on a wide range of industry benchmarks and offers new capabilities, including improved reasoning. We believe these are the best open source models of their [...] To develop a great language model, we believe it’s important to innovate, scale, and optimize for simplicity. We adopted this design philosophy throughout the Llama 3 project with a focus on four key ingredients: the model architecture, the pretraining data, scaling up pretraining, and instruction fine-tuning.\n\nModel architecture",
    "url": "https://ai.meta.com/blog/meta-llama-3/",
    "ext_info": null
  },
  {
    "id": 65,
    "gmt_created": "2025-09-15T01:27:25.890005",
    "gmt_modified": "2025-09-15T01:27:25.890005",
    "task_id": "7373043784122277888",
    "task_no": 65,
    "title": "The state of post-training in 2025 - Interconnects | Nathan Lambert",
    "type": "search_result",
    "content": "### 2. Post-training can be very expensive\n\nWhile still far cheaper than pretraining due to the price of GPUs, post-training costs have been growing rapidly. If we estimate the costs of post-training the Llama models, we could guess that the all-in costs for the models were about the following: _Note — numbers are based primarily on a combination of headcount and data costs with compute driving them even higher._\n\n   LLaMA (Q1 2023) <<$1M: instruction tuning only. [...] Llama 2 (Q3 2023) ~$10-20M: 1.4M preference pairs, RLHF, IFT, Safety, etc. and other costs not in the paper.\n\n   Llama 3.1 (Q3 2024) >$50M: similar preference data to Llama 2, a ~200-person post-training team, larger models, etc. The number could be much higher.",
    "url": "https://www.interconnects.ai/p/the-state-of-post-training-2025",
    "ext_info": null
  },
  {
    "id": 66,
    "gmt_created": "2025-09-15T01:27:31.671743",
    "gmt_modified": "2025-09-15T01:27:31.671743",
    "task_id": "7373043784122277888",
    "task_no": 66,
    "title": "Emergent Properties in Large Language Models (LLMs)",
    "type": "search_result",
    "content": "One emergent aspect here is the model’s innate ability to write correct code or structured queries that solve problems. In 2021, OpenAI trained Codex (a GPT-3 descendant) on code and it amazed people by generating working code from natural language. But even a purely generic model like PaLM 540B, not exclusively trained as a coder, showed strong coding abilities — it could write small programs to solve math problems, or use JSON format to query a database, etc., once it was large enough. [...] A particularly intriguing emergent property reported in 2023 is the semblance of a “Theory of Mind” in LLMs. Theory of Mind (ToM) is the ability to understand that others have beliefs, desires, and knowledge different from one’s own — a cognitive skill that humans typically develop around age 4–5. It was long thought to be uniquely human (or at least requiring advanced cognition). In early NLP models, anything like attributing mental states or understanding false beliefs was nonexistent. Yet, [...] From a benchmarking perspective, tasks that involve tool use or coding have shown big performance gains at large scale. The HumanEval benchmark (writing correct programs for given specs) went from near 0% for small models to 50%+ for Codex (GPT-3.5, 2021) and now over 80% for GPT-4 (2023). That improvement is not just training on more code — it’s also the model’s emergent ability to logically decompose a task into code. Anthropic’s Claude and Google’s PaLM 2 have similarly strong coding/tool",
    "url": "https://gregrobison.medium.com/emergent-properties-in-large-language-models-llms-deep-research-81421065d0ce",
    "ext_info": null
  },
  {
    "id": 67,
    "gmt_created": "2025-09-15T01:27:31.683086",
    "gmt_modified": "2025-09-15T01:27:31.683086",
    "task_id": "7373043784122277888",
    "task_no": 67,
    "title": "[2503.05788] Emergent Abilities in Large Language Models: A Survey",
    "type": "search_result",
    "content": "> Abstract:Large Language Models (LLMs) are leading a new technological revolution as one of the most promising research streams toward artificial general intelligence. The scaling of these models, accomplished by increasing the number of parameters and the magnitude of the training datasets, has been linked to various so-called emergent abilities that were previously unobserved. These emergent abilities, ranging from advanced reasoning and in-context learning to coding and problem-solving, [...] Subjects:Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)\nCite as:arXiv:2503.05788 [cs.LG]\n(or arXiv:2503.05788v2 [cs.LG] for this version)\n\nFocus to learn more\n\n arXiv-issued DOI via DataCite\n\nSubmission history\n\n From: Leonardo Berti [view email] \n\n( Fri, 28 Feb 2025 01:20:01 UTC (1,018 KB)\n\n[v2] Fri, 14 Mar 2025 13:28:04 UTC (1,347 KB)\n\n\n   TeX Source\n   Other Formats\n\nImage 5: license iconview license\n\n Current browse context: \n\ncs.LG [...] [2503.05788] Emergent Abilities in Large Language Models: A Survey\n\nSkip to main content\n\nImage 1: Cornell University Logo\n\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\n\n \n\n Submitted on 28 Feb 2025 ([v1), last revised 14 Mar 2025 (this version, v2)]\n\nTitle:Emergent Abilities in Large Language Models: A Survey\n\nAuthors:Leonardo Berti, Flavio Giorgi, Gjergji Kasneci",
    "url": "https://arxiv.org/abs/2503.05788",
    "ext_info": null
  },
  {
    "id": 68,
    "gmt_created": "2025-09-15T01:27:31.693055",
    "gmt_modified": "2025-09-15T01:27:31.693055",
    "task_id": "7373043784122277888",
    "task_no": 68,
    "title": "[PDF] Emergent Abilities in Large Language Models: A Survey - arXiv",
    "type": "search_result",
    "content": "> arXiv:2503.05788v2 [cs.LG] 14 Mar 2025 2\n> Emergent Abilities in Large Language Models: A Survey\n> § IV.Emergent Abilities as In-Context Learning\n> § III. Emergent Abilities in Large Language Models\n> § II. In-the-Wild Definitions of Emergent Abilities\n> § I. Introduction\n> § V.Emergent Abilities of Large Reasoning Models\n> § VI.Emergent Behaviors in LLMs-Powered AI Agents\n> § VII.Emergent Harmful Behaviors in LLMs and AI Agents\n> § VIII.A Taxonomic Synthesis of Key Findings and Challenges [...] broader analysis of emergent abilities was conducted by Wei et al. [ 87 ], who examined multiple LLMs – GPT-3 [ 11 ], LaMDA [ 80 ], Gopher [ 64 ], PaLM [ 17 ], Chinchilla [ 35 ] – across different scaling metrics, such as model size and training computation. They categorized tasks into few-shot prompted tasks and those benefiting from augmented prompting techniques like chain-of-thought reasoning and fine-tuning. Their findings reinforced the idea that emergent behaviors are not only [...] input prompt, LLMs demonstrate a remarkable ability to generalize to new tasks without explicit fine-tuning. Not only do these models exhibit improved performance, but they also demonstrate unexpected behaviors, giving rise to emergent abilities that were not anticipated or present in smaller models. The correlation between the scale of language models, as measured by training compute and model parameters, and their efficacy in various downstream natural language processing (NLP) tasks has been",
    "url": "https://arxiv.org/pdf/2503.05788",
    "ext_info": null
  },
  {
    "id": 69,
    "gmt_created": "2025-09-15T01:27:37.114671",
    "gmt_modified": "2025-09-15T01:27:37.114671",
    "task_id": "7373043784122277888",
    "task_no": 69,
    "title": "What Is Instruction Tuning? | IBM",
    "type": "search_result",
    "content": "As the power of LLMs increases, the utility of LLM-generated instruction tuning datasets has similarly increased. A 2023 paper replicated the Alpaca fine-tuning paradigm—which fine-tuned LLaMA on InstructGPT-generated instructions—while repeating the process in parallel using GPT-4 to generate instruction. The resultant model, which they dubbed LLaMA-GPT4, significantly outperformed the Alpaca equivalent’s “Helpfulness” scores and came close to matching GPT-4 itself in measures of [...] As articulated in Google Research’s influential 2022 paper, “Finetuned Language Models are Zero-Shot Learners,” the goal of instruction tuning is to improve the ability of LLMs to respond to NLP instructions. To do so, instruction tuning “combines appealing aspects of both the pretrain–finetune and prompting paradigms.” In essence, by organically incorporating the principles of prompt engineering into supervised fine-tuning, instruction tuning reduces the amount of prompt engineering and [...] The utility of instruction tuning, like that of most fine-tuning techniques, lies in the fact that pre-trained LLMs are not optimized for conversations or instruction following. In a literal sense, LLMs do not answer a prompt: they only append text to it. Instruction tuning helps make that appended text more useful.",
    "url": "https://www.ibm.com/think/topics/instruction-tuning",
    "ext_info": null
  },
  {
    "id": 70,
    "gmt_created": "2025-09-15T01:27:37.126304",
    "gmt_modified": "2025-09-15T01:27:37.126304",
    "task_id": "7373043784122277888",
    "task_no": 70,
    "title": "A Comparative Analysis of Instruction Fine-Tuning Large Language ...",
    "type": "search_result",
    "content": "### 2.3 Instruction Fine-Tuning\n\nInstruction fine-tuning is a resource-efficient approach for adapting LLMs to specific tasks. It allows models to adjust to domain-specific needs without the need for extensive retraining [39, 68]. This technique has proven effective across domains like medicine, law, and social sciences, where fine-tuned models often outperform general-purpose models like GPT-4 [9, 21, 25, 29, 65, 70]. [...] In the finance domain, several studies have applied instruction fine-tuning to models like LLaMA using financial datasets. Xie et al.  fine-tuned LLaMA on 136 k task-specific instruction samples, including sentiment analysis, named-entity recognition, question answering, and stock movement prediction, achieving results comparable to GPT-4. Yang et al.  developed an end-to-end framework for training and deploying FinLLMs in the finance sector using Low-Rank Adaptation (LoRA) to fine-tune LLaMA [...] The “Vanilla” models represent the zero-shot performance of instruct models. “MT-Base-FT” denotes multi-task fine-tuned base models, while “MT-Instruct-FT” refers to multi-task fine-tuned instruct models. Baseline model results are sourced from BloombergGPT , AdaptLLM-7B , FinMA-7B , InvestLM , and FinGPT . The best-performing results are highlighted in bold.\n\nTable 4.\n\n|  |",
    "url": "https://dl.acm.org/doi/full/10.1145/3706119",
    "ext_info": null
  },
  {
    "id": 71,
    "gmt_created": "2025-09-15T01:27:37.136530",
    "gmt_modified": "2025-09-15T01:27:37.136530",
    "task_id": "7373043784122277888",
    "task_no": 71,
    "title": "Fine-Tuning Large Language Models for Specialized Use Cases",
    "type": "search_result",
    "content": "Instruction-tuning: Instruction-tuning involves fine-tuning a pretrained LLM to follow specific task instructions, such as translation, summarization, or question answering. For example, in translation, the model is trained on examples in which each input includes an instruction like “Translate the following sentence from English to French,” followed by an English sentence and its French translation. After fine-tuning, the model learns to follow translation instructions and can generalize to [...] reports using either zero-shot learning (for Flan-T5, Med-Alpaca, Llama-2, and Zephyr) or QLoRA fine-tuning (Llama-2 and Zephyr). Using a training data set of 95,506 echocardiography reports, the authors observed that EchoGPT, which is a Llama-2 model trained using instruction fine-tuning with QLoRA, outperformed other LLMs on critical performance metrics. In addition, when 4 echocardiography board-certified cardiologists were asked to rate reports generated by EchoGPT for 30 randomly selected [...] 2.\nSpecific task optimization: general purpose LLMs can be optimized for specific tasks (eg, health report summarization and disease detection from a report).\n\n   3.\nData efficiency: fine-tuning works well with smaller quantities of labeled data because it involves using pretrained LLM(s) trained on huge data sets.",
    "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11976015/",
    "ext_info": null
  }
]